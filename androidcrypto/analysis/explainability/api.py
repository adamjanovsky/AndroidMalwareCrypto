from typing import Optional
from dataclasses import dataclass, field
import joblib
import gc

import pandas as pd
import numpy as np
import shap
from sklearn.utils import resample

from .config import ExplainabilityConfig
from .utils import save_as_df_to_hdf, safe_load_df_from_hdf

# max data size to use for interventional method
INTERVENTIONAL_MAX_BACKGROUND_DATA_SIZE = 1000

# keys into h5 file
SHAP_VALUES_KEY = "shap_values"
SHAP_INTERACTION_VALUES_KEY = "shap_interaction_values"
SHAP_EXPECTED_VALUES_KEY = "shap_expected_values"
VALUES_KEY = "values"
TARGETS_KEY = "targets"
HASHES_KEY = "hashes"

@dataclass
class ExplainabilityOutput:

    shap_values: Optional[pd.DataFrame] = field(default=None)
    shap_interaction_values: Optional[pd.DataFrame] = field(default=None)
    values: Optional[pd.DataFrame] = field(default=None)
    shap_expected_values: Optional[pd.DataFrame] = field(default=None)
    targets: Optional[pd.DataFrame] = field(default=None)
    hashes: Optional[pd.DataFrame] = field(default=None)
    explainer: Optional[shap.TreeExplainer] = field(default=None)
    

def explain_based_on_config(config: ExplainabilityConfig, seed: int=42) -> ExplainabilityOutput:

    # load inputs
    print("Explainability: Loading data.")
    X = pd.read_csv(config.values_path, index_col=0, squeeze=True)
    model = joblib.load(config.model_path)
    y = pd.read_csv(config.target_path, index_col=0, squeeze=True) if config.target_path is not None else None

    # sample
    if config.sample_size > 0 and config.sample_size <= X.shape[0]:
        print("Explainability: Sampling data.")
        if y is not None:
            X, y = resample(X, y, n_samples=config.sample_size, replace=False, random_state=seed)
        else:
            X = resample(X, n_samples=config.sample_size, replace=False, random_state=seed)
    
    # index
    if config.indices:
        print("Explainability: Selecting data by indices.")
        X = X.iloc[config.indices,]
        if y is not None:
            y = y.iloc[config.indices,]
    
    # build explainer and possibly sample data for interventional method
    if config.feature_perturbation == "interventional" and X.shape[0] > INTERVENTIONAL_MAX_BACKGROUND_DATA_SIZE:
        background_data = resample(X, n_samples=INTERVENTIONAL_MAX_BACKGROUND_DATA_SIZE,
                                   replace=False, random_state=seed)
    else: 
        background_data = X # maybe draw a sample here of max 1000 samples
    print("Explainability: Building explainer.")
    explainer = shap.TreeExplainer(
        model,
        background_data, # data that is used when "interventional" and linearly scales
        feature_perturbation=config.feature_perturbation,
        model_output=config.model_output
    )

    # calculate shap values
    shap_values = None
    if config.calculate_shap_values:
        print("Explainability: Calculating shap values.")
        shap_values = explainer.shap_values(
            X,
            y=y, # can be None
            approximate=config.approximate,
            check_additivity=True # TODO maybe remove
        )

    # calculate shap interaction values
    shap_interaction_values = None
    if config.calculate_interaction_values:
        print("Explainability: Calculating shap interaction values.")
        shap_interaction_values = explainer.shap_interaction_values(
            X,
            y=y
        )

    # saving
    shap_values_out_df = None
    if shap_values is not None:
        print("Explainability: Saving shap values.")
        shap_values_out_df = save_as_df_to_hdf(shap_values, config.output_path, SHAP_VALUES_KEY)

    shap_interaction_values_out_df = None
    if shap_interaction_values is not None:
        print("Explainability: Saving shap interaction values.")
        shap_interaction_values_out_df = save_as_df_to_hdf(shap_interaction_values, config.output_path,
                                                           SHAP_INTERACTION_VALUES_KEY)

    print("Explainability: Saving shap expected values.")
    shap_expected_values_out_df = save_as_df_to_hdf(explainer.expected_value, config.output_path,
                                                    SHAP_EXPECTED_VALUES_KEY)
    
    if config.save_values:
        print("Explainability: Saving values.")
        values_out_df = save_as_df_to_hdf(X, config.output_path, VALUES_KEY)
    else: 
        values_out_df = pd.DataFrame(X) 
    
    targets_out_df = None
    if y is not None:
        if config.save_targets:
            print("Explainability: Saving targets.")
            targets_out_df = save_as_df_to_hdf(y, config.output_path, TARGETS_KEY)
        else: 
            targets_out_df = pd.DataFrame(y)
    
    if config.save_hashes:
        print("Explainability: Saving hashes.")
        hashes_out_df = save_as_df_to_hdf([*X.index], config.output_path, HASHES_KEY)
    else:
        hashes_out_df = pd.DataFrame([*X.index])

    if config.output_explainer_path is not None:
        print("Explainability: Saving explainer.")
        explainer.save(config.output_explainer_path)

    return ExplainabilityOutput(
        shap_values=shap_values_out_df, 
        shap_interaction_values=shap_interaction_values_out_df, 
        shap_expected_values=shap_expected_values_out_df,
        explainer=explainer,
        values=values_out_df,
        targets=targets_out_df,
        hashes=hashes_out_df
    )


def load_explainability_output(path: str, explainer_path: Optional[str]) -> Optional[ExplainabilityOutput]:

    return ExplainabilityOutput(
        shap_values=safe_load_df_from_hdf(path, SHAP_VALUES_KEY), 
        shap_interaction_values=safe_load_df_from_hdf(path, SHAP_INTERACTION_VALUES_KEY), 
        shap_expected_values=safe_load_df_from_hdf(path, SHAP_EXPECTED_VALUES_KEY), 
        explainer=shap.TreeExplainer.load(explainer_path) if explainer_path is not None else None,
        values=safe_load_df_from_hdf(path, VALUES_KEY), 
        targets=safe_load_df_from_hdf(path, TARGETS_KEY), 
        hashes=safe_load_df_from_hdf(path, HASHES_KEY), 
    )

