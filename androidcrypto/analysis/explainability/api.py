from typing import Optional, List, Union
from dataclasses import dataclass, field
import joblib
import gc
import pickle

import pandas as pd
import numpy as np
import shap
from sklearn.utils import resample

from androidcrypto.analysis.explainability.config import ExplainabilityConfig
from androidcrypto.analysis.explainability.utils import save_as_df_to_hdf, safe_load_df_from_hdf

# max data size to use for interventional method
INTERVENTIONAL_MAX_BACKGROUND_DATA_SIZE = 10000

# keys into h5 file
SHAP_VALUES_KEY = "shap_values"
SHAP_INTERACTION_VALUES_KEY = "shap_interaction_values"
SHAP_EXPECTED_VALUES_KEY = "shap_expected_values"
VALUES_KEY = "values"
TARGETS_KEY = "targets"
HASHES_KEY = "hashes"

@dataclass
class ExplainabilityOutput:

    shap_values: Optional[List[np.array]] = field(default=None) # dataframe for each output in a list
    shap_interaction_values: Optional[List[np.array]] = field(default=None) # dataframe for each sample
    values: Optional[pd.DataFrame] = field(default=None)
    shap_expected_values: Optional[pd.DataFrame] = field(default=None)
    targets: Optional[pd.DataFrame] = field(default=None)
    hashes: Optional[pd.DataFrame] = field(default=None)
    explainer: Optional[shap.TreeExplainer] = field(default=None)
    

def explain_based_on_config(config: ExplainabilityConfig, seed: int=42) -> ExplainabilityOutput:

    # load inputs
    print("Explainability: Loading data.")
    X = safe_load_df_from_hdf(config.hdf_path, "X_test").drop(columns=config.columns_to_ignore, errors="ignore")
    model = joblib.load(config.model_path)
    y = safe_load_df_from_hdf(config.hdf_path, "y_test").squeeze("columns")

    # index
    X_explain = X
    if config.indices:
        print("Explainability: Selecting data by indices.")
        X_explain = X_explain.iloc[config.indices,]
        y = y.iloc[config.indices,]

    # sample
    if config.sample_size > 0 and config.sample_size <= X_explain.shape[0]:
        print("Explainability: Sampling data.")
        X_explain, y = resample(X_explain, y, n_samples=config.sample_size, replace=False, random_state=seed)
    
    # build explainer and possibly sample data for interventional method
    # NOTE: here original data is resampled to keep the same sample accross various parts of data indexed
    if config.feature_perturbation == "interventional" and X.shape[0] > INTERVENTIONAL_MAX_BACKGROUND_DATA_SIZE:
        background_data = np.array(resample(X, n_samples=INTERVENTIONAL_MAX_BACKGROUND_DATA_SIZE,
                                   replace=False, random_state=seed).values, dtype=np.dtype("float64"))
    else: 
        background_data = None
        
    del X # for memory reasons delete X as it is no longer needed
    gc.collect()
    
    print("Explainability: Building explainer.")
    explainer = shap.TreeExplainer(
        model,
        background_data, # data that is used when "interventional" and linearly scales
        feature_perturbation=config.feature_perturbation,
        model_output=config.model_output
    )

    # calculate shap values
    shap_values = None
    if config.calculate_shap_values:
        print("Explainability: Calculating shap values.")
        shap_values = explainer.shap_values(
            X_explain,
            y=y,
            approximate=config.approximate,
            #check_additivity=True # This check wont pass if sample is used for interventional method with sample
        )

    # calculate shap interaction values
    shap_interaction_values = None
    if config.calculate_interaction_values:
        print("Explainability: Calculating shap interaction values.")
        shap_interaction_values = explainer.shap_interaction_values(
            X_explain,
            y=y
        )

    # saving
    shap_values_out_df = None
    if shap_values is not None:
        print("Explainability: Saving shap values.")
        shap_values_out_df = save_as_df_to_hdf(shap_values, config.output_path, SHAP_VALUES_KEY)

    shap_interaction_values_out_df = None
    if shap_interaction_values is not None:
        print("Explainability: Saving shap interaction values.")
        shap_interaction_values_out_df = save_as_df_to_hdf(shap_interaction_values, config.output_path,
                                                           SHAP_INTERACTION_VALUES_KEY)

    print("Explainability: Saving shap expected values.")
    shap_expected_values_out_df = save_as_df_to_hdf(explainer.expected_values, config.output_path,
                                                    SHAP_EXPECTED_VALUES_KEY)
    
    if config.save_values:
        print("Explainability: Saving values.")
        values_out_df = save_as_df_to_hdf(X_explain, config.output_path, VALUES_KEY, columns=X_explain.columns)
    else: 
        values_out_df = pd.DataFrame(X_explain, columns=X_explain.columns) 
    
    targets_out_df = None
    if y is not None:
        if config.save_targets:
            print("Explainability: Saving targets.")
            targets_out_df = save_as_df_to_hdf(y, config.output_path, TARGETS_KEY, columns=y.columns)
        else: 
            targets_out_df = pd.DataFrame(y, columns=y.columns)
    
    if config.save_hashes:
        print("Explainability: Saving hashes.")
        hashes_out_df = save_as_df_to_hdf([*X_explain.index], config.output_path, HASHES_KEY)
    else:
        hashes_out_df = pd.DataFrame([*X_explain.index])

    if config.output_explainer_path is not None:
        print("Explainability: Saving explainer.")
        with open(config.output_explainer_path, 'wb') as handle:
            pickle.dump(explainer, handle)

    return ExplainabilityOutput(
        shap_values=shap_values_out_df, 
        shap_interaction_values=shap_interaction_values_out_df, 
        shap_expected_values=shap_expected_values_out_df,
        explainer=explainer,
        values=values_out_df,
        targets=targets_out_df,
        hashes=hashes_out_df
    )


def load_explainability_output(path: str, explainer_path: Optional[str]=None) -> Optional[ExplainabilityOutput]:
    if explainer_path is not None:
        with open(explainer_path, 'rb') as handle:
            explainer = pickle.load(handle)
    else:
        explainer = None

    return ExplainabilityOutput(
        shap_values=[df.values for df in safe_load_df_from_hdf(path, SHAP_VALUES_KEY, dimension=3)], 
        shap_interaction_values=[df.values for df in safe_load_df_from_hdf(path, SHAP_INTERACTION_VALUES_KEY, dimension=3)], 
        shap_expected_values=safe_load_df_from_hdf(path, SHAP_EXPECTED_VALUES_KEY), 
        explainer=explainer,
        values=safe_load_df_from_hdf(path, VALUES_KEY), 
        targets=safe_load_df_from_hdf(path, TARGETS_KEY), 
        hashes=safe_load_df_from_hdf(path, HASHES_KEY), 
    )

