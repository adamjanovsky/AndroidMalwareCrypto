"""
File contains functions used for data loading.

Author: Dominik Macko
"""

import os
from functools import reduce
from typing import List, Tuple, Optional

import pandas as pd
from sklearn.model_selection import train_test_split

from androidcrypto.analysis.preparation.config import CleaningInputPath

SOURCE_COL_NAME = "metadata_source"

def load_cleaning_input_paths(paths: List[CleaningInputPath]) -> pd.DataFrame:
    """Loads all cleaning input paths."""

    dataframes = [load_json_data(p.path, benign=p.benign, source=p.source) for p in paths]

    df = reduce((lambda a, b: a.append(b)), dataframes)
    adjust_types(df)
    return df.drop(columns=["standard_third_party_packages"])


def load_json_data(file_path: str, benign: Optional[bool]=None, source: Optional[str]=None) -> pd.DataFrame:
    """Loads json data from given file path and parses nested 'metadata' json."""
    
    data = pd.read_json(file_path, orient="index")
    
    # load nested metadata dictionary
    data["metadata_apk_path"] = data["metadata"].apply(lambda x: x["apk_path"])
    data["metadata_dx_path"] = data["metadata"].apply(lambda x: x["dx_path"])
    data["metadata_dex_year"] = data["metadata"].apply(lambda x: x["dex_year"])
    data["metadata_euphony_type"] = data["metadata"].apply(lambda x: x["euphony_type"])
    data["metadata_euphony_name"] = data["metadata"].apply(lambda x: x["euphony_name"])
    data.drop(columns=["metadata"], inplace=True)

    if benign is not None:
        data["benign"] = benign
    if source is not None:
        data[SOURCE_COL_NAME] = source
    
    return data

def adjust_types(df : pd.DataFrame) -> None:
    """Adjusts types of whole dataframe after loading."""
    
    df["metadata_apk_path"] = df["metadata_apk_path"].astype("string")
    df["metadata_dx_path"] = df["metadata_dx_path"].astype("string")
    df["metadata_dex_year"] = pd.Categorical(df["metadata_dex_year"], ordered=True)
    df["metadata_euphony_type"] = pd.Categorical(df["metadata_euphony_type"])
    df["metadata_euphony_name"] = pd.Categorical(df["metadata_euphony_name"])

def load_all_json_data(malicious_file_paths : List[str],
                       benign_file_paths : List[str]
                      ) -> pd.DataFrame:
    """Loads all json data from different files and places them together into a dataframe."""
        
    # Load all dataframes from all paths
    dataframes = [*map(lambda path: load_json_data(path, False), malicious_file_paths)] + [
        *map(lambda path: load_json_data(path, True), benign_file_paths)]
                  
    # Concat all dataframes into one
    df = reduce((lambda a, b: a.append(b)), dataframes)
                  
    adjust_types(df)
    return df.drop(columns=["standard_third_party_packages"])

def get_all_json_paths(dir_path: str) -> List[str]:
    """Gets all json paths from a given directory."""
    
    paths = [os.path.join(dir_path, f) for f in os.listdir(dir_path)]
    return [p for p in paths if p.endswith(".json") and os.path.isfile(p)]
    

def load_all_json_data_from_dir(dir_path: str,
                                benign_subdir_name: str="benign",
                                malicious_subdir_name: str="malicious"
                               ) -> pd.DataFrame:
    """Loads all json data from a given directory."""
    
    malicious_paths = get_all_json_paths(os.path.join(dir_path, malicious_subdir_name))
    benign_paths = get_all_json_paths(os.path.join(dir_path, benign_subdir_name))
    return load_all_json_data(malicious_paths, benign_paths)

def train_valid_test_split(df: pd.DataFrame,
                           test_size : int,
                           valid_size : int,
                           random_state: int = 42,
                           shuffle : bool = True
                          ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """Splits the dataset into train, validation and test dataframes and returns them in given order."""
    
    train_size = 1 - (test_size + valid_size)
    train_df, valid_and_test_df = train_test_split(df, train_size=train_size,
                                                   random_state=random_state, shuffle=shuffle)
    
    coeff = 1 / (test_size + valid_size)
    valid_df, test_df = train_test_split(valid_and_test_df, test_size=test_size * coeff,
                                         random_state=random_state, shuffle=shuffle)
    return train_df, valid_df, test_df
