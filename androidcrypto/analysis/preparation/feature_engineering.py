"""
File contains class used for feature engineering.

Author: Dominik Macko
"""

from .config import FeatureEngineeringConfig
from .feature_scaling import FeatureScalerTransformer
from typing import Union, Any, List, Dict, Tuple, Optional

import numpy as np
import pandas as pd
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer
from sklearn.feature_extraction import DictVectorizer
from sklearn.preprocessing import OrdinalEncoder
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split

from .utils import (
    list_contains,
    concat_sublists,
    create_frequency_dict,
    get_records_values_series,
    extract_constructors,
    get_constructor_series,
    get_constructor_file_sequences,
    get_file_line_index_list,
    get_file_line_index_list_series,
    get_file_line_index_frequency_dict,
    get_file_line_index_frequency_dict_series,
    get_line_index_overlap_count,
    get_line_index_overlap_count_series,
    get_max_line_index_overlap_count,
    get_max_line_index_overlap_count_series,
)

class FeatureEngineeringTransformer(BaseEstimator, TransformerMixin):
    """Class that is used to engineer new features."""
    
    def __init__(self,
                 records_min_ngrams: int=1,
                 records_max_ngrams: int=5,
                 strong_hashes: List[str]=[
                     "SHA-256",
                     "SHA-384",
                     "SHA-512",
                     "SHA-1",
                     "SHA-224"],
                 use_third_party_packages_vector: bool=False,
                 years: List[str]=["2012", "2013", "2014", "2015", "2016", "2017", "2018", "2019", "2020"],
                 min_df: Union[float, int]=0.005, # 0.0 is 0%, 0.005 is 0.5%
                 max_df: Union[float, int]=0.995, # 1.0 is 100%, 0.995 is 99.5%
                 use_tfidf: bool=False,
                 use_year: bool=False
        ):
        """Initializes feature engineering transformer.
        
        records_min_ngrams - specifies minimum n_grams to use for crypto api records sequences
        records_max_ngrams - specifies the maximum n_grams to use for crypto api records sequences
        strong_hashes - specifies which hashes are considered strong
        use_third_party_packages_vector - whether third party packages should be vectorized
        years - years to be used
        min_df - minimum document frequency
        max_df - maximum document frequency
        use_tfidf - whether to use tfidf or not
        use_year - whether to use year or nor
        """
        
        self._records_min_ngrams: int = records_min_ngrams
        self._records_max_ngrams: int = records_max_ngrams
        lower_strong_hashes = [h.lower() for h in strong_hashes]
        self._strong_hashes: List[str] = lower_strong_hashes + [
            h.replace("-", "") for h in lower_strong_hashes] + [h.replace("-", "_") for h in lower_strong_hashes]
        self._use_third_party_packages_vector: bool = use_third_party_packages_vector
        self._use_year = use_year
        
        self._year_encoder: OrdinalEncoder = OrdinalEncoder(categories=[years], dtype=np.intc)
            
        self._third_party_packages_vectorizer = CountVectorizer(
            lowercase=False, tokenizer=self._dummy_func, preprocessor=self._dummy_func,
            token_pattern=None, binary=True, dtype=np.bool_, min_df=min_df, max_df=max_df
        )

        self._third_party_crypto_libs_vectorizer = CountVectorizer(
            lowercase=False, tokenizer=self._dummy_func, preprocessor=self._dummy_func,
            token_pattern=None, binary=True, dtype=np.bool_, min_df=2
        )
        
        if use_tfidf:
            self._crypto_imports_vectorizer_pipeline = Pipeline([
                ("dict_vectorizer", DictVectorizer()),
                ("tfidf_transformer", TfidfTransformer())
            ])

            self._crypto_api_records_vectorizer = TfidfVectorizer(
                lowercase=False, analyzer=self._crypto_api_records_analyzer, 
                min_df=min_df, max_df=max_df
            )
        else:
            self._crypto_imports_vectorizer_pipeline = Pipeline([
                ("dict_vectorizer", DictVectorizer()),
            ])

            self._crypto_api_records_vectorizer = CountVectorizer(
                lowercase=False, analyzer=self._crypto_api_records_analyzer, 
                min_df=min_df, max_df=max_df
            )
            
        
    def fit(self,
            X: pd.DataFrame,
            y: Optional[Union[pd.DataFrame, pd.Series]]=None
           ) -> "FeatureEngineeringTransformer":
        """Fits transformer to given training data."""
        
        self._fit_native_imports(X)
        self._fit_third_party_packages(X)
        self._fit_third_party_crypto_libs(X)
        self._fit_crypto_imports(X)
        self._fit_crypto_api_records(X)
        self._fit_metadata_dex_year(X)
        return self
        
    def transform(self,
                  X: pd.DataFrame,
                  y: Optional[Union[pd.DataFrame, pd.Series]]=None
                 ) -> pd.DataFrame:
        """Transforms given training data, must be used only after being fit."""
        
        X_ = X.copy() # copy just in case to not modify the supplied dataframe
        X_ = self._transform_native_imports(X_)
        X_ = self._transform_third_party_packages(X_)
        X_ = self._transform_third_party_crypto_libs(X_)
        X_ = self._transform_crypto_imports(X_)
        X_ = self._transform_crypto_api_records(X_)
        X_ = self._transform_metadata_dex_year(X_)
        return X_
    
    @staticmethod
    def _dummy_func(x: Any) -> Any:
        """Dummy function that returns the passed object without changing anything"""
        
        return x
    
    @staticmethod
    def _add_names(X: Union[List[Any], np.array], prefix: str,
                   feature_names: Union[List[str], np.array],
                   index: List[str]) -> pd.DataFrame:
        """Creates dataframe by adding names as columns and keeping given index."""
        
        feature_names = [f"{prefix}={name}" for name in feature_names]
        return pd.DataFrame(X, columns=feature_names, index=index)
    
    @staticmethod
    def _vectorize(X: pd.DataFrame, vectorizer: Union[CountVectorizer, TfidfVectorizer, DictVectorizer],
                   prefix: str) -> pd.DataFrame:
        """Vectorizes given dataframe, using fitted vectorizer and using prefix for feature names."""
        
        matrix = vectorizer.transform(X).todense()
        feature_names = [*vectorizer.get_feature_names()]
        index = X.index.to_list()
        return FeatureEngineeringTransformer._add_names(matrix, prefix, feature_names, index)
    
    def _fit_native_imports(self, X: pd.DataFrame) -> None:
        """Fits native imports by doing nothing in this version."""
        
        return None
    
    def _transform_native_imports(self, X: pd.DataFrame) -> pd.DataFrame:
        """Transforms native imports by dropping them and adding binary (bool) flag to signal if they are used."""
        
        X["uses_native_imports"] = X["native_imports"].apply(lambda l: len(l) != 0).astype("bool")
        return X.drop(columns=["native_imports"])
    
    def _fit_third_party_packages(self, X: pd.DataFrame) -> None:
        """Fits third party packages vectorizer if usage of third party packages is specified in constructor."""
        
        if self._use_third_party_packages_vector:
             self._third_party_packages_vectorizer.fit(X["third_party_packages"])
    
    def _transform_third_party_packages(self, X: pd.DataFrame) -> pd.DataFrame:
        """Transform third party packages into optional vector and ratio of third party crypto libs to overall packages."""
        
        if not self._use_third_party_packages_vector:
            return X.drop(columns=["third_party_packages"])
        
        X["third_party_packages_sqrt_count"] = X["third_party_packages"].apply(len).apply(np.sqrt)
        vectorized = self._vectorize(X["third_party_packages"],
                                     self._third_party_packages_vectorizer, "third_party_package")
        X.drop(columns=["third_party_packages"], inplace=True)
        return pd.concat([X, vectorized], axis=0)
    
    def _fit_third_party_crypto_libs(self, X: pd.DataFrame) -> None:
        """Fits third party crypto libs column, calculating values for vector."""
        
        self._third_party_crypto_libs_vectorizer.fit(X["third_party_crypto_libs"])
    
    def _transform_third_party_crypto_libs(self, X: pd.DataFrame) -> pd.DataFrame:
        """Transforms third party crypto libs column based on fit info into vector and into square root count."""
        
        third_party_crypto_lib_count = X["third_party_crypto_libs"].apply(len)
        X["uses_one_third_party_crypto_lib"] = (third_party_crypto_lib_count == 1).astype("bool")
        #X["uses_atleast_two_third_party_crypto_lib"] = (third_party_crypto_lib_count > 1).astype("bool")
        
        vectorized = self._vectorize(X["third_party_crypto_libs"],
                                     self._third_party_crypto_libs_vectorizer, "third_party_crypto")
        X.drop(columns=["third_party_crypto_libs"], inplace=True)
        return pd.concat([X, vectorized], axis=1)
    
    def _fit_crypto_imports(self, X: pd.DataFrame) -> None:
        """Fits crypto imports tf-idf vectorizer."""
        
        self._crypto_imports_vectorizer_pipeline.fit(X["crypto_imports"])
    
    def _transform_crypto_imports(self, X: pd.DataFrame) -> pd.DataFrame:
        """Transform crypto imports, creating total and unique counts and tf-idf vector."""
        
        X["crypto_imports_total_sqrt_count"] = X["crypto_imports"].apply(
            lambda x: sum([*x.values()])).apply(np.sqrt) # TODO MAYBE NORMALIZE BY BINARY SIZE
        X["crypto_imports_unique_sqrt_count"] = X["crypto_imports"].apply(
            lambda x: len([*x.keys()])).apply(np.sqrt)
        #X["crypto_imports_total_unique_sqrt_ratio"] = (X["crypto_imports_total_sqrt_count"] / X["crypto_imports_unique_sqrt_count"]).replace((np.inf, np.nan), 0)
        
        # TODO MAYBE NORMALIZE BY BINARY SIZE
        transformed = self._crypto_imports_vectorizer_pipeline.transform(X["crypto_imports"].values).todense()
        feature_names = [*self._crypto_imports_vectorizer_pipeline[0].get_feature_names()]
        vectorized = self._add_names(transformed, "crypto_imports", feature_names, X.index.to_list())
        
        X.drop(columns=["crypto_imports"], inplace=True)
        return pd.concat([X, vectorized], axis=1)

        
    def _fit_metadata_dex_year(self, X: pd.DataFrame) -> None:
        """Fits metadata dex year ordinal encoder."""
        
        if self._use_year:
            self._year_encoder.fit(X[["metadata_dex_year"]])
    
    def _transform_metadata_dex_year(self, X: pd.DataFrame) -> pd.DataFrame:
        """Transforms metadata dex year by using fitted ordinal encoder."""
        
        if self._use_year:
            encoded = self._year_encoder.transform(X[["metadata_dex_year"]])
            X["metadata_dex_year"] = encoded
            return X
        return X.drop(columns=["metadata_dex_year"])
    
    def _crypto_file_record_analyzer(self, constructor_sequence: List[str], out: List[str]) -> None:
        """Analyzes constructor sequence for a single file and appends result into out."""
        
        # iterate over each constructor
        for i in range(len(constructor_sequence)):
            window = []
            # construct each possible ngram and append it (if n == 1 then do only one iteration)
            for ngram in range(self._records_max_ngrams):
                if i + ngram >= len(constructor_sequence):
                    break
                window.append(constructor_sequence[i + ngram])
                # if min ngram is not met then continue loading api calls
                if len(window) < self._records_min_ngrams:
                    continue
                out.append(" ".join(window))                
    
    def _crypto_api_records_analyzer(self, constructor_sequences: List[List[str]]) -> List[str]:
        """Analyzes api records constructor file sequences and returns ngrams.
        
        Each file (list of strings) is treated as a sequence that can be used for creation of n-grams.
        note: when n > 1 then n-grams are only from one given file - each file is a separate sequence.
        note: inspired by https://towardsdatascience.com/hacking-scikit-learns-vectorizers-9ef26a7170af
        """
        
        result = []
        for sequence in constructor_sequences:
            self._crypto_file_record_analyzer(sequence, result)
        return result
        
    
    def _fit_crypto_api_records(self, X: pd.DataFrame) -> None:
        """Fits crypto api records vectorizer using custom analyzer."""
        
        sequences = [*get_constructor_file_sequences(X["crypto_api_records"]).values]
        self._crypto_api_records_vectorizer.fit(sequences)
        
    def _engineer_crypto_api_records_calls(self,
                                           X: pd.DataFrame,
                                           constructors: pd.Series,
                                           constructor_sequences: pd.Series
                                          ) -> None:
        """Engineers crypto api records calls.
        
        Specifically engineer: count of files (classes), api calls count, their ratio,
        maximum constructors on single line, constructor overlap, unique constructors count
        """
        
        # TODO MAYBE NORMALIZE BY BINARY SIZE
        # this represents classes where crypto is used
        X["file_sqrt_count"] = X["crypto_api_records"].apply(lambda x: len(list(x.keys()))).apply(np.sqrt)
        
        api_calls_per_file = constructor_sequences.apply(lambda sequences: [len(s) for s in sequences])
        #X["min_sqrt_api_calls_per_file"] = api_calls_per_file.apply(
        #    lambda calls: min(calls, default=0)).apply(np.sqrt)
        #X["max_sqrt_api_calls_per_file"] = api_calls_per_file.apply(
        #    lambda calls: max(calls, default=0)).apply(np.sqrt)
        #X["mean_sqrt_api_calls_per_file"] = api_calls_per_file.apply(
        #    lambda calls: np.mean(calls) if len(calls) > 0 else 0).apply(np.sqrt)

        X["api_calls_sqrt_count"] = constructors.apply(len).apply(np.sqrt)
        X["unique_api_calls_sqrt_count"] = constructors.apply(lambda l: len([*set(l)])).apply(np.sqrt)
        
        #X["api_file_calls_sqrt_ratio"] = (X["file_sqrt_count"] / X["api_calls_sqrt_count"]
        #                                 ).replace((np.nan, np.inf), 0)
        
        #X["max_line_overlap"] = get_max_line_index_overlap_count_series(X["crypto_api_records"])
        #X["line_overlap_sqrt_count"] = get_line_index_overlap_count_series(X["crypto_api_records"]).apply(np.sqrt)
        
        
    def _engineer_crypto_api_records_hashing_usage(self, 
                                                   X: pd.DataFrame,
                                                   constructors: pd.Series
                                                  ) -> None:
        """Engineer crypto api records hashing usage.
        
        Specifically information about ise of hashing, MD5, SHA1, weak and strong hashes, their ratios..
        """
        
        hashes = constructors.apply(
            lambda l: [*filter(lambda c: c.startswith("MessageDigest.getInstance"), l)]).apply(
            lambda l: [*map(lambda c: c.lower(), l)])
        hash_count = hashes.apply(len)
        X["hashing_sqrt_count"] = hash_count.apply(np.sqrt)
        # TODO MAYBE remove these (they are contained in ngrams) and only leave hashing and strong hashing count
        # (possibly change it to weak hashing count)
        X["uses_MD5"] = constructors.apply(
            lambda l: list_contains(l, ["md5"])).astype("bool") 
        X["uses_SHA1"] = constructors.apply(
            lambda l: list_contains(l, ["sha1", "sha-1", "sha_1"])).astype("bool")
        X["uses_SHA256"] = constructors.apply(
            lambda l: list_contains(l, ["sha256", "sha-256", "sha_256"])).astype("bool")

        strong_hash_count = constructors.apply(lambda l: len([*filter(lambda c: c in self._strong_hashes, l)]))
        weak_hash_count = hash_count - strong_hash_count
        X["strong_hashing_sqrt_count"] = strong_hash_count.apply(np.sqrt)
        #X["uses_weak_hash"] = (weak_hash_count != 0).astype("bool")
        #X["uses_strong_hash"] = (strong_hash_count != 0).astype("bool")
        #X["strong_to_weak_hash_ratio"] = (strong_hash_count / weak_hash_count).replace((np.nan, np.inf), 0)
        #X["strong_to_overall_hash_ratio"] = (strong_hash_count / hash_count).replace((np.nan, np.inf), 0)
        #X["weak_to_overall_hash_ratio"] = (weak_hash_count / hash_count).replace((np.nan, np.inf), 0)
                    
        
    def _engineer_crypto_api_records_symmetric_crypto_usage(self,
                                                            X: pd.DataFrame,
                                                            constructors: pd.Series
                                                           ) -> None:
        """Engineer symmetric crypto api records usage - specifically AES, DES, weak ECB/PCKS7Padding mode"""
    
        # TODO maybe remove these they are in the ngrams
        X["uses_AES"] = constructors.apply(lambda l: list_contains(l, ["aes", "AES"])).astype("bool")
        X["uses_DES"] = constructors.apply(lambda l: list_contains(l, ["des", "DES"])).astype("bool")

        X["uses_ECB/PKCS7Padding"] = constructors.apply(
            lambda l: len([*filter(lambda c: c == "Cipher.getInstance(\"DES" or (
                c == "Cipher.getInstance(\"AES" or "ECB/PKCS7Padding" in c), l)]) != 0).astype("bool")
        
        
    def _engineer_crypto_api_records_public_key_crypto_usage(self,
                                                             X: pd.DataFrame,
                                                             constructors: pd.Series
                                                            ) -> None:
        """Engineer information whether app uses public key crypto."""
    
        X["uses_public_crypto_RSA"] = constructors.apply(
            lambda l: len([*filter(lambda c: "Cipher" in c and "RSA" in c, l)]) != 0).astype("bool")
        return None
        
    
    def _engineer_crypto_api_records_digital_signature_usage(self,
                                                             X: pd.DataFrame,
                                                             constructors: pd.Series
                                                            ) -> None:
        """Engineers information about digital signature usage.
        
        note: https://docs.oracle.com/en/java/javase/15/docs/specs/security/standard-names.html#signature-algorithms
        """

        signatures = constructors.apply(lambda l: [*filter(lambda c: c.startswith("Signature"), l)])
        digital_signature_count = signatures.apply(len)
        # TODO maybe remove these - they are in the ngrams
        X["uses_digital_signature"] = (digital_signature_count != 0).astype("bool")
        X["uses_SHA1withRSA"] = signatures.apply(
            lambda l: len([*filter(lambda c: "sha1withrsa" in c.lower(), l)]) != 0).astype("bool")
        X["uses_SHA256withRSA"] = signatures.apply(
            lambda l: len([*filter(lambda c: "sha256withrsa" in c.lower(), l)]) != 0).astype("bool")
        X["uses_nonRSA_digital_signature"] = signatures.apply(
            lambda l: len([*filter(lambda c: "dsa" in c.lower(), l)]) != 0).astype("bool")
        X["uses_nonhashed_signature"] = signatures.apply(
            lambda l: len([*filter(lambda c: "nonewith" in c.lower(), l)]) != 0).astype("bool")
        X["uses_weakhashed_signature"] = signatures.apply(
            lambda l: len([*filter(lambda c: "md5" in c.lower(), l)])).astype("bool")
        

    def _engineer_crypto_api_records_MAC_usage(self,
                                               X: pd.DataFrame,
                                               constructors: pd.Series
                                              ) -> None:
        """Engineer information about MAC usage, HMAC usage and whether app uses weak hashed Hmac"""

        # TODO maybe remove these - they are in the ngrams
        macs = constructors.apply(lambda l: [*filter(lambda c: c.startswith("Mac"), l)])
        MAC_count = macs.apply(lambda l: len(l))
        X["uses_MAC"] = (MAC_count != 0).astype("bool")
        X["uses_HmacSHA1"] = macs.apply(
            lambda l: len([*filter(lambda c: "hmacsha1" in c.lower(), l)]) != 0).astype("bool")
        X["uses_HmacSHA256"] = macs.apply(
            lambda l: len([*filter(lambda c: "hmacsha256" in c.lower(), l)]) != 0).astype("bool")
        X["uses_weak_hashed_Hmac"] = macs.apply(
            lambda l: len([*filter(lambda c: "md5" in c.lower(), l)]) != 0).astype("bool")
 
    def _engineer_crypto_api_records_key_agreements_protocols_usage(self,
                                                                    X: pd.DataFrame,
                                                                    constructors: pd.Series
                                                                   ) -> None:
        """Engineer information about key agreement protocols usage."""

        X["uses_key_agreements_protocols"] = constructors.apply(
            lambda l: len([*filter(lambda c: c.startswith("KeyAgreement"), l)]) != 0).astype("bool")

    def _engineer_crypto_api_records_PRNG_usage(self, X: pd.DataFrame, constructors: pd.Series) -> None:
        """Engineer information about pseudo random generators usage."""

        X["uses_PRNG"] = constructors.apply(
            lambda l: len([*filter(lambda c: c.startswith("SecureRandom"), l)]) != 0).astype("bool")
        X["uses_SHA1PRNG"] = constructors.apply(
            lambda l: len([*filter(lambda c: c.startswith("SHA1PRNG"), l)]) != 0).astype("bool")
    
    def _transform_crypto_api_records(self, X: pd.DataFrame) -> pd.DataFrame:
        """Transforms crypto api records of given dataframe.
        
        Specifically vectorize and engineer calls, hashing usage, public key usage, symmetric crypto usage,
        digital signature usage, MAC usage, key agreement protocols usage, PRNG usage
        """
        
        constructors = get_constructor_series(X["crypto_api_records"])
        constructor_sequences = get_constructor_file_sequences(X["crypto_api_records"])
        
        self._engineer_crypto_api_records_calls(X, constructors, constructor_sequences)
        self._engineer_crypto_api_records_hashing_usage(X, constructors)
        self._engineer_crypto_api_records_symmetric_crypto_usage(X, constructors)
        self._engineer_crypto_api_records_public_key_crypto_usage(X, constructors)
        self._engineer_crypto_api_records_digital_signature_usage(X, constructors)
        self._engineer_crypto_api_records_MAC_usage(X, constructors)
        self._engineer_crypto_api_records_key_agreements_protocols_usage(X, constructors)
        self._engineer_crypto_api_records_PRNG_usage(X, constructors)
        
        # TODO maybe normalize by binary size
        vectorized = self._vectorize(constructor_sequences,
                                     self._crypto_api_records_vectorizer, "crypto_api_records")
        
        X.drop(columns=["crypto_api_records"], inplace=True)
        return pd.concat([X, vectorized], axis=1)

def feature_engineering_based_on_config(
    dfs: Optional[Dict[str, Tuple[pd.DataFrame, pd.DataFrame]]],
    config: FeatureEngineeringConfig
    ) -> Dict[str, Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]]:
    """Engineers features based on a config file.
    
    returns train_X, train_y, test_X, test_y
    """

    print("Feature engineering: Starting.")
    result = {}
    used_dfs = dfs
    if dfs is None:
        used_dfs = {}

    for task_name, task_config in config.task_configs.items():
        # load if None
        if task_name not in dfs:
            if not task_config.input_features_path:
                print(f"Feature engineering: Task {task_name}: No data from previous step and input features path not defined.")
                continue
            if not task_config.input_target_path:
                print(f"Feature engineering: Task {task_name}: No data from previous step and input target path not defined.")
                continue
            print(f"Feature engineering: Task {task_name}: Loading data.")
            used_dfs[task_name] = (
                pd.read_json(task_config.input_features_path, orient="index"),
                pd.read_json(task_config.input_target_path, orient="index"))

        X, y = used_dfs[task_name]
        # split
        print(f"Feature engineering: Task {task_name}: Splitting data.")
        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=config.train_set_ratio, random_state=42)

        # create Transformer and engineer
        print(f"Feature engineering: Task {task_name}: Engineering features.")
        transformer = FeatureEngineeringTransformer(
            records_min_ngrams=config.min_ngrams,
            records_max_ngrams=config.max_ngrams,
            strong_hashes=config.strong_hashes,
            use_third_party_packages_vector=config.use_third_party_packages,
            min_df=config.min_document_frequency,
            max_df=config.max_document_frequency,
            use_tfidf=config.use_tfidf,
            use_year=config.use_year
        )
        transformer.fit(X_train)
        X_train = transformer.transform(X_train)
        X_test = transformer.transform(X_test)

        # optionally scale
        if config.scale_features:
            print(f"Feature engineering: Task {task_name}: Scaling features.")
            scaler = FeatureScalerTransformer([*X_train.select_dtypes(exclude="bool").select_dtypes(include="number").columns])
            scaler.fit(X_train, y_train)
            X_train = scaler.transform(X_train)
            X_test = scaler.transform(X_test)

        # optionally save for each of the tasks
        if task_config.output_train_features_path:
            print(f"Feature engineering: Task {task_name}: Saving train features.")
            X_train.to_csv(task_config.output_train_features_path)
        if task_config.output_train_features_path:
            print(f"Feature engineering: Task {task_name}: Saving train targets.")
            y_train.to_csv(task_config.output_train_features_path)
        if task_config.output_train_features_path:
            print(f"Feature engineering: Task {task_name}: Saving test features.")
            X_test.to_csv(task_config.output_train_features_path)
        if task_config.output_train_features_path:
            print(f"Feature engineering: Task {task_name}: Saving test targets.")
            y_test.to_csv(task_config.output_train_features_path)

        result[task_name] = (X_train, y_train, X_test, y_test)

    return result
    