"""
File contains functions that can be used to train LGBM (GBDT).

Author: Dominik Macko
"""

from typing import Union, Tuple, Any, Optional

import numpy as np
import pandas as pd
import optuna
import lightgbm
import joblib
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import LabelEncoder

from androidcrypto.analysis.training.config import TrainingTaskConfig
from androidcrypto.analysis.training.utils import compute_balanced_class_weights_array, is_multiclass

def train_lgbm_cv(*args: Any, **kwargs: Any) -> Any:
    """Simple crossvalidation wrapper inspired by optuna.integration._lightgbm_tuner.train"""

    booster = optuna.integration.lightgbm.LightGBMTunerCV(*args, **kwargs)
    booster.run()
    return booster.get_best_booster()

def train_lgbm_gbdt_optuna(train_X: Union[pd.DataFrame, np.array],
                           train_y: Union[pd.Series, np.array],
                           study_name: str="lgbm",
                           n_jobs: int=8,
                           seed: int=42,
                           early_stopping_rounds: int=50,
                           objective: str="multiclass",
                           lgbm_metric: str="softmax",
                           time_budget: int=24*60*60,
                           cv_splits: int=5
                          ) -> Tuple[lightgbm.Booster, optuna.study.Study]:
    """Trains LGBM (GBDT) using Optuna
    
    train_X - training set features
    train_y - training set targets
    study_name - name of Optuna study
    n_jobs - thread to use
    seed - seed to use
    early_stopping_rounds - rounds to early stop after
    objective - LGBM objective to use
    lgbm_metric - LGBM metric to use
    time_budget - maximum time budget in seconds
    
    returns (LGBM booster, Optuna study)
    """

    train_weights = compute_balanced_class_weights_array(train_y)
    dtrain = lightgbm.Dataset(train_X, label=train_y, weight=train_weights)
    classes = len(np.unique(train_y))
    classes = classes if classes > 2 else 1

    params = {
        "objective": objective,
        "metric": lgbm_metric,
        "is_unbalance": True,
        "verbosity": -1,
        "boosting_type": "gbdt",
        "seed": seed,
        "objective_seed": seed,
        "num_class": classes,
        "n_jobs": n_jobs,
        "early_stopping_round": early_stopping_rounds,
        "folds": StratifiedKFold(n_splits=cv_splits, random_state=seed, shuffle=True)
    }

    study = optuna.create_study(
        study_name=study_name
    )

    model = train_lgbm_cv(
        params,
        dtrain,
        verbose_eval=100,
        study=study,
        time_budget=time_budget,
        optuna_seed=seed
    )

    return (
        model,
        study
    )

def train_lgbm_based_on_config(train_X: pd.DataFrame, 
                               train_y: pd.Series, 
                               test_X: Optional[pd.DataFrame], 
                               task_config: TrainingTaskConfig,
                               cv_splits: int,
                               threads: int
                               ) -> Tuple[lightgbm.Booster, Optional[pd.DataFrame]]:
    
    print("Training: LGBM: Starting training.")
    multiclass = is_multiclass(train_y)
    if multiclass:
        objective = "multiclass",
        lgbm_metric ="softmax"
        study_name = "lgbm_multi"
    else:
        objective = "binary",
        lgbm_metric = "binary_logloss"
        study_name = "lgbm"
    enc = LabelEncoder()
    train_y_enc = enc.fit_transform(train_y)
    model, _ = train_lgbm_gbdt_optuna(train_X, train_y_enc, cv_splits=cv_splits, objective=objective,
                                      lgbm_metric=lgbm_metric, n_jobs=threads, study_name=study_name)
    if task_config.output_model_path:
        print("Training: LGBM: Saving model.")
        joblib.dump(model, task_config.output_model_path)

    test_pred = None
    if test_X is not None:
        print("Training: LGBM: Predicting test labels.")
        if multiclass:
            test_pred = enc.inverse_transform([np.argmax(l) for l in model.predict(test_X)])
        else:
            test_pred = enc.inverse_transform(np.rint(model.predict(test_X)).astype("int8"))
        if task_config.output_prediction_path:
            print("Training: LGBM: Saving predicted test labels.")
            pd.Series(test_pred).to_csv(task_config.output_prediction_path)
    
    return model, test_pred
