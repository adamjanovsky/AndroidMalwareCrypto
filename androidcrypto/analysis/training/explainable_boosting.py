"""
File contains functions that can be used to train Explainable Boosting Classifier.

Author: Dominik Macko
"""

from interpret.glassbox import ExplainableBoostingClassifier

from typing import Dict, Any, Union, Callable, Tuple, Optional, List

import numpy as np
import pandas as pd
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import make_scorer, get_scorer
import optuna
import joblib

from .generic import train_model, train_randomizedsearchcv_model, train_gridsearchcv_model
from .config import TrainingTaskConfig
from .utils import is_multiclass, get_feature_types, compute_balanced_class_weights_array, cross_val_score_weighted

def explainable_boosting_best_params_surroundings(best_params: Dict[str, Any]) -> Dict[str, Any]:
    """Get best parameters surroundings for random forest."""
    
    max_bins = best_params["max_bins"]
    max_interaction_bins = best_params["max_interaction_bins"]
    outer_bags = best_params["outer_bags"]
    inner_bags = best_params["inner_bags"]
    learning_rate = best_params["learning_rate"]
    min_samples_leaf = best_params["min_samples_leaf"]
    max_leaves = best_params["max_leaves"]

    best_params["max_bins"] = [max_bins - max_bins // 4, max_bins, max_bins + max_bins * 2]
    best_params["max_interaction_bins"] = [max_interaction_bins - max_interaction_bins // 4, max_interaction_bins, max_interaction_bins + max_interaction_bins * 2]
    best_params["outer_bags"] = [outer_bags - outer_bags // 4, outer_bags, outer_bags + outer_bags * 2]
    best_params["inner_bags"] = [inner_bags - inner_bags // 4, inner_bags, inner_bags + inner_bags * 2]
    best_params["learning_rate"] = [1/2 * learning_rate, learning_rate, 3/2 * learning_rate]
    best_params["min_samples_leaf"] = [min_samples_leaf - 1, min_samples_leaf, min_samples_leaf + 1]
    best_params["max_leaves"] = [max_leaves - 1, max_leaves, max_leaves + 1]

    return best_params

def train_explainable_boosting(train_X: np.array,
                               train_y: np.array,
                                scoring: Union[str, Callable[[Any, np.array, np.array], int]]="f1_macro",
                                n_jobs: int=8,
                                verbose: int=3,
                                seed: int=42,
                                cv_splits: int=5,
                                mains: Union[str, List[int]]="all", # features to train on,
                                validation_size: float=0.1,
                                early_stopping_rounds: int=50,
                                max_rounds: int=5000,
                                feature_names: Optional[List[str]]=None,
                                feature_types: Optional[List[str]]=None 
                       ) -> Tuple[ExplainableBoostingClassifier, pd.DataFrame]:
    """Trains explainable boosting classifier by searching for optimal alpha smoothing term.
    
    train_X - training set features
    train_y - training set targets
    scoring - scikit scoring function to use
    n_jobs - threads to use
    seed - seed to use
    verbose - scikit verbose level
    
    returns (model, history dataframe)
    """
    
    grid = {
        "max_bins": [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024],
        "max_interaction_bins": [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024],
        "binning": ["uniform", "quantile", "quantile_humanized"],
        "outer_bags": [0, 1, 2, 4, 8, 16, 32, 64, 128, 256],
        "inner_bags": [0, 1, 2, 4, 8, 16, 32, 64, 128, 256],
        "learning_rate": [0.00001, 0.0001, 0.001, 0.01, 0.1],
        "min_samples_leaf": [3, 5, 7, 9, 11, 13, 15, 17, 19, 21],
        "max_leaves": [3, 5, 7, 9, 11, 13, 15, 17, 19, 21],
    }
    return train_model(
        ExplainableBoostingClassifier(
            n_jobs=n_jobs, 
            random_state=seed,
            validation_size=validation_size,
            early_stopping_rounds=early_stopping_rounds,
            max_rounds=max_rounds,
            mains=mains,
            feature_names=feature_names,
            feature_types=feature_types
            ),
        train_randomizedsearchcv_model,
        train_gridsearchcv_model,
        grid,
        explainable_boosting_best_params_surroundings,
        train_X,
        train_y,
        scoring=scoring,
        n_jobs=n_jobs,
        verbose=verbose,
        seed=seed,
        cv_splits=cv_splits
    )

class ExplainableBoostingObjective:
    """Random Forest Objective used for Optuna library to optimize"""
    
    def __init__(
        self,
        train_X: Union[pd.DataFrame, np.array],
        train_y: Union[pd.Series, np.array],
        scorer: Union[Callable[[Union[np.array, pd.Series], Union[np.array, pd.Series]], float], str],
        n_jobs: int=8,
        seed: int=42,
        cv_splits: int=5,
        mains: Union[str, List[int]]="all", # features to train on,
        validation_size: float=0.1,
        early_stopping_rounds: int=50,
        max_rounds: int=5000,
        feature_names: Optional[List[str]]=None,
        feature_types: Optional[List[str]]=None
    ):
        """Initializes the objective
        
        train_X - training set features
        train_y - training set targets
        scorer - callable used to score performance (maximize)
        n_jobs - threads to use
        seed - random seed to use
        class_weight - class weight passed to RandomFOrestClassifier init
        n_estimators - trees to use
        """

        self.train_X = train_X
        self.train_y = train_y
        self.weights = compute_balanced_class_weights_array(train_y)
        self.scorer = get_scorer(scorer) if isinstance(scorer, str) else make_scorer(scorer)
        self.n_jobs = n_jobs
        self.seed = seed
        self.cv = StratifiedKFold(n_splits=cv_splits, random_state=seed, shuffle=True)
        self.mains = mains
        self.validation_size = validation_size
        self.early_stopping_rounds = early_stopping_rounds
        self.max_rounds = max_rounds
        self.feature_names = feature_names
        self.feature_types = feature_types
        
    def best_model_callback(self,
                            study: optuna.study.Study,
                            trial: optuna.trial.Trial
                           ) -> None:
        """Callback that can be used to store the best model in the study."""
        
        if study.best_trial.number == trial.number:
            study.set_user_attr(key="best_model", value=trial.user_attrs["model"])
    
    def __call__(self, trial: optuna.trial.Trial) -> float:
        """Calls the objective to optimize it.
        
        trial - Optuna trial
        
        returns the scorer score on validation set
        """
        
        max_bins = 2 ** trial.suggest_int("max_bins_exponent", 1, 11)
        max_interaction_bins = 2 ** trial.suggest_int("max_interaction_bins_exponent", 1, 11)
        binning = trial.suggest_categorical("binning", ["uniform", "quantile", "quantile_humanized"])
        outer_bags = trial.suggest_int("outer_bags", 0, 256)
        inner_bags = trial.suggest_int("inner_bags", 0, 256)
        learning_rate = trial.suggest_float("learning_rate", 0.00001, 1.0, log=True)
        min_samples_leaf = trial.suggest_int("min_samples_leaf", 2, 25)
        max_leaves = trial.suggest_int("max_leaves", 2, 25)

        emb = ExplainableBoostingClassifier(
            n_jobs=self.n_jobs, 
            random_state=self.seed,
            validation_size=self.validation_size,
            early_stopping_rounds=self.early_stopping_rounds,
            max_rounds=self.max_rounds,
            mains=self.mains,
            feature_names=self.feature_names,
            feature_types=self.feature_types,
            # now start hyperparameters
            max_bins=max_bins,
            max_interaction_bins=max_interaction_bins,
            binning=binning,
            outer_bags=outer_bags,
            inner_bags=inner_bags,
            learning_rate=learning_rate,
            min_samples_leaf=min_samples_leaf,
            max_leaves=max_leaves,
        )
        
        metric = cross_val_score_weighted(dl, self.train_X, self.train_y, self.weights, scoring=self.scorer, cv=self.cv)
        emb.fit(self.train_X, self.train_y)
        trial.set_user_attr(key="model", value=emb)
        return metric

def train_explainable_boosting_optuna(train_X: Union[pd.DataFrame, np.array],
                               train_y: Union[pd.Series, np.array],
                               scorer: Union[Callable[[Union[np.array, pd.Series], Union[np.array, pd.Series]], float], str],
                               study_name: str="explainable_boosting",
                               n_jobs: int=8,
                               seed: int=42,
                               n_trials: int=100,
                               cv_splits: int=5,
                               mains: Union[str, List[int]]="all", # features to train on,
                               validation_size: float=0.1,
                               early_stopping_rounds: int=50,
                               max_rounds: int=5000,
                               feature_names: Optional[List[str]]=None,
                               feature_types: Optional[List[str]]=None
                              ) -> Tuple[ExplainableBoostingClassifier, optuna.study.Study]:
    """Trains Random Forest using Optuna.
    
    train_X - training set features
    train_y - training set targets
    scorer - callable that is used to score the performance
    study_name - Optuna study name
    n_jobs - threads to use
    seed - random seed to use
    n_trials - maximum trials
    
    returns (Random Forest model, Optuna study)
    """
    
    study = optuna.create_study(
        direction="maximize",
        study_name=study_name,
        sampler=optuna.samplers.TPESampler(seed=seed)
    )
    
    objective = ExplainableBoostingObjective(
        train_X,
        train_y,
        scorer,
        seed=seed,
        cv_splits=cv_splits,
        mains=mains,
        validation_size=validation_size,
        early_stopping_rounds=early_stopping_rounds,
        max_rounds=max_rounds,
        feature_names=feature_names,
        feature_types=feature_types
    )
    
    study.optimize(
        objective,
        n_trials=n_trials,
        n_jobs=n_jobs,
        callbacks=[objective.best_model_callback],
        gc_after_trial=True
    )
    
    return (
        study.user_attrs["best_model"],
        study    
    )

def train_explainable_boosting_based_on_config(train_X: pd.DataFrame, 
                                        train_y: pd.Series, 
                                        test_X: Optional[pd.DataFrame], 
                                        task_config: TrainingTaskConfig,
                                        cv_splits: int
                                        ) -> Tuple[ExplainableBoostingClassifier, Optional[pd.DataFrame]]:
    
    print("Training: Explainable Boosting: Starting training.")
    metric = "f1_macro" if is_multiclass(train_y) else "f1"
    model, _ = train_explainable_boosting_optuna(train_X.values,
                                                 train_y.values,
                                                 metric, 
                                                 cv_splits=cv_splits, 
                                                 feature_names=[*train_X.columns],
                                                 feature_types=get_feature_types(train_X)
                                                 )
    if task_config.output_model_path:
        print("Training: Explainable Boosting: Saving model.")
        joblib.dump(model, task_config.output_model_path)

    test_pred = None
    if test_X is not None:
        print("Training: Explainable Boosting: Predicting test labels.")
        test_pred = model.predict(test_X)
        if task_config.output_prediction_path:
            print("Training: Explainable Boosting: Saving predicted test labels.")
            pd.Series(test_pred).to_csv(task_config.output_prediction_path)
    
    return model, test_pred
