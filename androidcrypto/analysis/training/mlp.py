"""
File contains functions that can be used to train Keras MLP.

Author: Dominik Macko
"""

from typing import Union, Dict, Optional, Tuple, Any, Callable, List

import numpy as np
import pandas as pd
import tensorflow as tf
import tensorflow_addons as tfa
import optuna

from .utils import compute_balanced_class_weights
from .config import TrainingTaskConfig

class KerasMLPObjective:
    """Keras MLP objective to optimize for Optuna study."""
    
    def __init__(
        self,
        train_X: Union[pd.DataFrame, np.array],
        train_y: Union[pd.Series, pd.DataFrame, np.array],
        classes: int,
        metric,
        seed: int=42,
        class_weight: Optional[Dict[int, float]]=None,
        output_activation: str="softmax",
        loss_function: str="categorical_crossentropy",
        epochs: int=20,
        batch_size: int=32,
        verbose: int=1,
        early_stopping: int=3,
        mode: str="max"
    ):
        """Initializes the objective
        
        train_X - training set features
        train_y - training set targets
        validation_X - validation set features
        validation_y - validation set targets
        classes - count of classes
        metric - metric to use
        seed - random seed to use
        class_weight - class weight passed to Keras fit
        output_activation - output layer activation function
        loss_function - loss function to use
        epochs - epochs to train
        batch_size - batch size for training
        verbose - KEras verbose level
        early_stopping - count of epochs to use as early stopping
        """
        
        tf.random.set_seed(seed)
        np.random.seed(seed)
        self.train_X = train_X
        self.train_y = train_y
        self.seed = seed
        self.class_weight = class_weight
        self.output_activation = output_activation
        self.classes = classes
        self.metric = metric
        self.metric_name = metric.name
        self.loss_function = loss_function
        self.epochs = epochs
        self.batch_size = batch_size
        self.verbose = verbose
        self.early_stopping = early_stopping
        self.mode = mode
        
        self.trial_models: Dict[int, tf.keras.Sequential] = {}
        
    def _create_model(self, trial: optuna.trial.Trial) -> tf.keras.Model:
        """Creates a Keras MLP model from the trial."""
        
        n_layers = trial.suggest_int("n_layers", 1, 3)
        hidden_activation = trial.suggest_categorical(f"activation_function", ["selu", "relu"])
        model = tf.keras.Sequential()
        
        # add hidden layers
        for n in range(n_layers):
            num_hidden = trial.suggest_categorical(f"n_units_l{n}", [16, 32, 64, 128, 256, 512])
            
            name_dense = f"dense_{n}_{hidden_activation}_{n_layers}_{num_hidden}"
            if hidden_activation == "selu":
                model.add(
                    tf.keras.layers.Dense(num_hidden,
                                          activation=hidden_activation,
                                          kernel_initializer="lecun_normal",
                                          **{"name":name_dense})
                )
            else:
                model.add(
                    tf.keras.layers.Dense(num_hidden, 
                                          activation=hidden_activation,
                                          kernel_initializer="he_normal",
                                          **{"name":name_dense})
                )
        output_layer_neurons = self.classes if self.classes > 2 else 1
        name_output = f"output_{self.output_activation}_{n_layers}_{output_layer_neurons}"
        model.add(tf.keras.layers.Dense(output_layer_neurons,
                                        activation=self.output_activation,
                                        **{"name":name_output}))
                                        
        learning_rate = trial.suggest_categorical("learning_rate", [0.00001, 0.0001, 0.001, 0.01])
        model.compile(
            loss=self.loss_function,
            optimizer=tf.keras.optimizers.Nadam(learning_rate=learning_rate),
            metrics=[self.metric],
        )

        return model
    
    def remove_bad_models_callback(self,
                                   study: optuna.study.Study,
                                   trial: optuna.trial.Trial
                                  ) -> None:
        """Callback used to remove bad models from the dictionary storage."""
        
        best = study.best_trial.number
        keys = [*self.trial_models.keys()]
        for k in keys:
            if k != best:
                self.trial_models.pop(k, None)
        
    def __call__(self, trial: optuna.trial.Trial) -> float:
        """Call the optimization objective."""
        
        # clear from previous session
        tf.keras.backend.clear_session()
        
        model = self._create_model(trial)
        monitor = f"val_{self.metric_name}"
        
        history = model.fit(
            self.train_X,
            self.train_y,
            batch_size=self.batch_size,
            epochs=self.epochs,
            validation_data=(self.validation_X, self.validation_y),
            verbose=self.verbose,
            class_weight=self.class_weight,
            callbacks=[
                tf.keras.callbacks.EarlyStopping(patience=self.early_stopping, monitor=monitor, mode=self.mode),
                optuna.integration.TFKerasPruningCallback(trial, monitor),
            ]
        )
        self.trial_models[trial.number] = model
        return history.history[monitor][-1]
        
def train_MLP_optuna(train_X: Union[pd.DataFrame, np.array],
                     train_y: Union[pd.Series, pd.DataFrame, np.array],
                     validation_X: Union[pd.DataFrame, np.array],
                     validation_y: Union[pd.Series, pd.DataFrame, np.array],
                     classes: int,
                     metric_name: str="f1",
                     study_name: str="MLP",
                     seed: int=42,
                     n_trials: int=50,
                     class_weights: Optional[Dict[int, float]]=None,
                     output_activation: str="softmax",
                     loss_function: str="categorical_crossentropy",
                     epochs: int=20,
                     batch_size: int=32,
                     verbose: int=1,
                     early_stopping: int=3,
                     n_jobs: int=1,
                     warmup_steps: int=5
                    ) -> Tuple[tf.keras.Sequential, optuna.study.Study]:
    """Trains MLP using Optuna library.
    
    train_X - training set features
    train_y - training set targets
    validation_X - validation set features
    validation_y - validation set targets
    classes - count of classes
    metric_name - metric's name to use
    study_name - name of optuna study
    seed - random seed to use
    n_trials - trials to run
    class_weight - class weight passed to Keras fit
    output_activation - output layer activation function
    loss_function - loss function to use
    epochs - epochs to train
    batch_size - batch size for training
    verbose - KEras verbose level
    early_stopping - count of epochs to use as early stopping
    n_jobs - threads to use
    warmup_steps - warmup trials before any pruning
    
    returns (MLP Keras model, optuna study)
    """
    
    study = optuna.create_study(
        direction="maximize",
        study_name=study_name,
        pruner=optuna.pruners.HyperbandPruner(),
        sampler=optuna.samplers.TPESampler(seed=seed)
    )
    
    if metric_name == "f1":
        if classes <= 2:
            metric = tfa.metrics.F1Score(1, threshold=0.5)
        else:
            metric = tfa.metrics.F1Score(classes, average="macro")
    elif metric_name == "cohen_kappa":
        metric = tfa.metrics.CohenKappa(classes)
    else: 
        metric = tf.keras.metrics.get(metric_name.capitalize())
    
    objective = KerasMLPObjective(
        train_X,
        train_y,
        validation_X,
        validation_y,
        classes,
        metric,
        class_weight=class_weights,
        seed=seed,
        output_activation=output_activation,
        loss_function=loss_function,
        epochs=epochs,
        batch_size=batch_size,
        verbose=verbose
    )
    
    study.optimize(
        objective,
        n_trials=n_trials,
        n_jobs=n_jobs,
        callbacks=[objective.remove_bad_models_callback],
        gc_after_trial=True
    )
    
    return (
        objective.trial_models[study.best_trial.number],
        study
    )

def train_mlp_based_on_config(train_X: pd.DataFrame, 
                               train_y: pd.Series, 
                               test_X: Optional[pd.DataFrame], 
                               task_config: TrainingTaskConfig,
                               cv_splits: int
                               ) -> Tuple[tf.keras.Sequential, Optional[pd.DataFrame]]:
    
    raise NotImplementedError("MLP config training not implemented rn.")
