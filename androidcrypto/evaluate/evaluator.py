import logging
import argparse
from datetime import datetime
import os
import androidcrypto.constants as constants
import androidcrypto.helpers as helpers

import json
import pprint
from copy import deepcopy
import pandas as pd
import numpy as np
import yaml
import re
import logging
import copy
import matplotlib
import seaborn as sns
import matplotlib.pyplot as plt
import os
from dataclasses import dataclass, field, InitVar
from pathlib import Path
import yaml
import re
import copy
import sys
import math
import pickle


class ExperimentEvaluationError(Exception):
    pass


@dataclass
class ExperimentResults:
    json_results_path: str = field(init=False)
    config_path: str = field(init=False)
    meta_path: str = field(init=False)
    n_samples: int = field(init=False)
    year: int = field(init=False)
    recorded_api: dict = field(init=False)
    folder: InitVar[str]

    def __post_init__(self, folder: str):
        self.meta_path = os.path.join(folder, 'results/meta.yml')
        self.config_path = os.path.join(folder, 'config')

        if len(config_files := list(os.listdir(self.config_path))) != 1:
            raise ExperimentEvaluationError('Unxpected files in config path of {folder}. Expected only one file.')
        else:
            self.config_path = os.path.join(self.config_path, config_files[0])

        with open(self.config_path, 'r') as yaml_handle:
            config_stream = yaml.load(yaml_handle, Loader=yaml.FullLoader)

        self.recorded_api = config_stream['evaluate']['categories']

        self.year = config_stream['download']['start_year']

        self.json_results_path = os.path.join(folder, 'results/' + str(self.year) + '.json')
        with open(self.meta_path, 'r') as yaml_handle:
            meta_stream = yaml.load(yaml_handle, Loader=yaml.FullLoader)
        self.n_samples = int(meta_stream['n_samples'])


class Evaluator:
    cols = ['sha256', 'year', 'euphony_name', 'euphony_type', 'java_crypto_libs', 'native_crypto_libs', 'third_party_packages',
            'crypto_api_imports', 'crypto_api_records']

    def __init__(self, input_folder, report_path, api_definition_path, show=True, save_files=True):
        sns.set_style("whitegrid")
        self.input_path = input_folder
        self.show = show
        self.save_files = save_files
        self.report_path = report_path
        self.experiments = None

        with open(api_definition_path, 'r') as yaml_handle:
            self.api_definition = yaml.load(yaml_handle, Loader=yaml.FullLoader)

        self.df = None
        self.df_records = None
        self.df_imports = None
        self.size_comparison = None
        self.java_libs = None
        self.native_libs = None

        self.dataset_sizes = None
        self.dset_size = 0
        self.norm_factor = None
        self.summary_handle = None

        self.exp_config = None

        self.prepare_report_dir()
        if self.save_files is True:
            self.summary_handle = open(os.path.join(self.report_path, 'summary.md'), 'w')


    def __repr__(self):
        return 'Evaluator=' + str(self.__dict__)

    def evaluate(self):
        self.get_exp_results()
        self.get_experiments_config()
        self.init_dataframes()
        self.analyze_third_party_packages()

        self.df, self.size_comparison = self.analyze_empty_samples()

        euph_folder = os.path.join(self.report_path, 'euphony')
        if not os.path.exists(euph_folder):
            os.mkdir(euph_folder)
        self.plot_euphony(self.df, euph_folder)

        #self.plot_size_comparison(size_comparison, os.path.join(self.report_path, 'crypto_usage_proportion.png'))

        self.analyze_third_party_libs()
        self.get_imports_dataframe()
        self.get_records_dataframe()
        self.analyze_crypto_api_imports()

        #self.plot_imports_comparison(self.df_imports, os.path.join(self.report_path, 'imports_comparison.png'), self.norm_factor)

        # We should not need this any more...
        self.df = self.df.drop(columns='crypto_api_imports')
        self.df = self.df.drop(columns='crypto_api_records')

        val_counts_years = self.df_records.groupby('year').trigger.value_counts()
        with open(os.path.join(self.report_path, 'groupby_records.html'), 'w') as f:
            f.write(constants.HTML_STRING.format(table=pd.DataFrame(val_counts_years).to_html(classes='mystyle')))

        val_counts = self.df_records.trigger.value_counts()
        with open(os.path.join(self.report_path, 'sum_records.html'), 'w') as f:
            f.write(constants.HTML_STRING.format(table=pd.DataFrame(val_counts).to_html(classes='mystyle')))

        #self.analyze_memory_usage()

        ciphers_folder = os.path.join(self.report_path, 'ciphers')
        if not os.path.exists(ciphers_folder):
            os.mkdir(ciphers_folder)

        #self.plot_all_categories()
        # self.plot_all_subcategories()

        self.summary_handle.close()

    def prepare_report_dir(self):
        if not os.path.exists(self.report_path):
            os.mkdir(self.report_path)

    def get_exp_results(self):
        try:
            self.experiments = [ExperimentResults(os.path.join(self.input_path, x)) for x in os.listdir(self.input_path)]
        except Exception as e:
            logging.error(f'Failed to parse folder with results: {e}')

    def get_experiments_config(self):
        """
        Assuming that every experiment has the same config on tasks, categories, etc.
        """
        with open(self.experiments[0].config_path) as handle:
            self.exp_config = yaml.load(handle, Loader=yaml.FullLoader)

    def init_dataframes(self):
        json_data = helpers.merge_jsons(*[exp.json_results_path for exp in self.experiments])
        self.dataset_sizes = {exp.year: exp.n_samples for exp in self.experiments}
        self.dset_size = sum(self.dataset_sizes.values())
        self.norm_factor = {key: val / 10000 for key, val in self.dataset_sizes.items()}

        data = [(sample_id, rec['metadata']['dex_year'], rec['metadata']['euphony_name'], rec['metadata']['euphony_type'],
                 rec['third_party_crypto_libs'], rec['native_imports'], rec['third_party_packages'], rec['crypto_imports'], rec['crypto_api_records'])
                for sample_id, rec in json_data.items()]
        self.df = pd.DataFrame(data, columns=self.cols).set_index('sha256')
        self.df = helpers.introduce_missing_vals(self.df, ['euphony_name', 'euphony_type', 'java_crypto_libs', 'native_crypto_libs', 'third_party_packages', 'crypto_api_imports', 'crypto_api_records'])

        # self.df = helpers.drop_adware(df)

    def analyze_third_party_packages(self):
        def print_report(stream):
            print('Statistics about third-party packages', file=stream)
            print(f'{n_full} out of {third_party.shape[0]} ({(n_full / third_party.shape[0] * 100):.2f}%) apks contain at least one third party package', file=stream)
            print(f'On average, each apk contains: {third_party.n_third_party.mean()} third-party packages.', file=stream)
            helpers.print_delim(stream)

        def count_packages(x):
            if isinstance(x, float):
                return 0
            else:
                return len(x)

        self.df['n_third_party'] = self.df.third_party_packages.apply(count_packages)
        third_party = self.df.drop(
            columns=['euphony_name', 'euphony_type', 'java_crypto_libs', 'native_crypto_libs', 'crypto_api_imports',
                     'crypto_api_records'])
        n_empty = third_party.loc[third_party.n_third_party == 0].shape[0]
        n_full = third_party.shape[0] - n_empty

        if self.show is True:
            print_report(sys.stdout)

        if self.save_files is True:
            print_report(self.summary_handle)
        #ax = sns.distplot(third_party.n_third_party, kde=False)

    def analyze_empty_samples(self):
        def print_report(stream):
            print('General statistics about dataset', file=stream)
            print(f'Number of analyzed samples: {size_comparison.all_samples.sum():,}', file=stream)
            print('How many of analyzed samples use some crypto...', file=stream)
            print(size_comparison.to_markdown(), file=stream)
            helpers.print_delim(stream)

        df_clean = self.df.dropna(how='all', subset=['java_crypto_libs', 'native_crypto_libs', 'crypto_api_imports', 'crypto_api_records'])

        all_samples_size = self.df.groupby('year').size().to_frame(name='all_samples')
        containing_crypto_size = df_clean.groupby('year').size().to_frame(name='containing_crypto')

        size_comparison = pd.concat([all_samples_size, containing_crypto_size], axis=1)
        size_comparison['empty_samples'] = size_comparison.all_samples - size_comparison.containing_crypto
        size_comparison['percentage_containing_crypto'] = (size_comparison.containing_crypto / size_comparison.all_samples) * 100

        if self.show is True:
            print_report(sys.stdout)

        if self.save_files is True:
            print_report(self.summary_handle)

        return df_clean, size_comparison

    def analyze_third_party_libs(self):
        def print_report(stream):
            print('Usage of third party cryptographic libraries', file=stream)
            print('Java libraries:', file=stream)

            if self.java_libs.empty:
                print('No java cryptographic libraries were found', file=stream)
            else:
                print(self.java_libs.groupby('year').lib_name.value_counts(), file=stream)
                helpers.print_delim(stream)
                print('Prevalence of java libraries per year:', file=stream)
                print(self.java_libs.groupby('year').sha256.nunique(), file=stream)
                helpers.print_delim(stream)
                print('Overall popularity of java libraries:', file=stream)
                print(self.java_libs.lib_name.value_counts(), file=stream)

            print('Native libraries:', file=stream)
            if self.native_libs.empty:
                print('No native cryptographic libraries were found', file=stream)
            else:
                print(self.native_libs.groupby('year').lib_name.value_counts(), file=stream)

            helpers.print_delim(stream)

        self.java_libs = helpers.get_third_party_libs_df(self.df, 'java_crypto_libs')
        self.native_libs = helpers.get_third_party_libs_df(self.df, 'native_crypto_libs')

        if self.show is True:
            print_report(sys.stdout)
        if self.save_files is True:
            print_report(self.summary_handle)

    def analyze_crypto_api_imports(self):
        def print_report(stream):
            print('Number of crypto API imports per sample:', file=stream)
            print(self.df.crypto_imports_sum.describe().to_markdown(), file=stream)
            print(f'Out of {n_theoretical_imports} analyzed classes, the following number is actually imported:', file=stream)
            print(practical_imports, file=stream)
            helpers.print_delim(stream)

        def get_sum_of_crypto_imports_per_sample(sample_imports):
            if isinstance(sample_imports, dict):
                return sum([x for x in sample_imports.values()])
            else:
                return 0  # should be always np.nan

        self.df['crypto_imports_sum'] = self.df.crypto_api_imports.apply(get_sum_of_crypto_imports_per_sample)

        with open(self.experiments[0].config_path, 'r') as handle:
            filestream = yaml.load(handle, Loader=yaml.FullLoader)
        n_theoretical_imports = len(filestream['evaluate']['imports'])
        practical_imports = self.df_imports.groupby('year').api_class.nunique()

        if self.show is True:
            print_report(sys.stdout)
        if self.save_files is True:
            print_report(self.summary_handle)

    def get_imports_dataframe(self):
        lst = []
        for row in self.df.itertuples():
            if isinstance(row.crypto_api_imports, dict):
                lst.extend([(row.Index, row.year, api_class, n_ocurrences) for api_class, n_ocurrences in
                            row.crypto_api_imports.items()])
        self.df_imports = pd.DataFrame(lst, columns=['sha256', 'year', 'api_class', 'class_count'])

    def get_records_dataframe(self):
        lst = []
        for row in self.df.itertuples():
            if isinstance(row.crypto_api_records, dict):
                for key, records in row.crypto_api_records.items():
                    lst.extend([(row.Index, row.year, key, r[0], r[1], r[2]) for r in records])
        self.df_records = pd.DataFrame(lst, columns=['sha256', 'year', 'class_name', 'trigger', 'line', 'line_number'])
        self.deduplicate_and_fix_missing_triggers('/Users/adam/phd/projects/CryptoMalware/experiments/new_triggers.yml')

    def deduplicate_and_fix_missing_triggers(self, path_to_new_triggers):
        # Quite time consuming, taking up to 1 minute
        def apply_new_triggers(row):
            if not row.trigger.lower() in default_triggers:
                return row
            for t in new_triggers[row.trigger]:
                if t.lower() in row.line.lower():
                    row.trigger = t
                    return row
            return row

        with open(path_to_new_triggers, 'r') as yaml_handle:
            new_triggers = yaml.load(yaml_handle, Loader=yaml.FullLoader)
        default_triggers = [x.lower() for x in list(new_triggers.keys())]

        self.df_records = self.df_records.loc[self.df_records.trigger != 'Cipher']  # Manual fix of broken trigger, not needed
        self.df_records = self.df_records.drop_duplicates(subset=['sha256', 'class_name', 'line_number'],
                                                                    keep='last')
        self.df_records = self.df_records.apply(apply_new_triggers, axis=1)
        self.fix_broken_triggers()

    def fix_broken_triggers(self):
        def map_sha(x):
            if x == 'MessageDigest.getInstance("SHA1':
                return 'MessageDigest.getInstance("SHA-1'
            elif x == 'MessageDigest.getInstance("SHA256':
                return 'MessageDigest.getInstance("SHA-256'
            else:
                return x

        self.df_records.trigger = self.df_records.trigger.map(map_sha)

    def analyze_memory_usage(self):
        def print_report(stream):
            print(f'Total df size in megabytes: {df_usage}', file=stream)
            print(f'Total df_imports size in megabytes: {df_imports_usage}', file=stream)
            print(f'Total df_records size in megabytes: {df_records_usage}', file=stream)
            helpers.print_delim(stream)

        df_usage = self.df.memory_usage(deep=True).sum() / 1000000
        df_records_usage = self.df_records.memory_usage(deep=True).sum() / 1000000
        df_imports_usage = self.df_imports.memory_usage(deep=True).sum() / 1000000

        if self.show is True:
            print_report(sys.stdout)
        if self.save_files is True:
            print_report(self.summary_handle)

    def plot_size_comparison(self, categories):
        triggers = [set(helpers.flatten(self.api_definition[x], include_obfuscated=True)) for x in categories]
        cat_cols = ['contains_' + cat for cat in categories]
        cat_perc_cols = ['percentage_' + cat for cat in categories]

        for cat, trigg, cat_col, cat_perc_col in zip(categories, triggers, cat_cols, cat_perc_cols):
            self.df_records[cat_col] = self.df_records.trigger.isin(trigg)
            df_cat = self.df_records.loc[self.df_records[cat_col] == True]
            df_cat = df_cat.groupby('year').sha256.nunique().to_frame(name=cat_col)
            self.size_comparison = self.size_comparison.join(df_cat)
            self.size_comparison[cat_perc_col] = (self.size_comparison[cat_col] / self.size_comparison.all_samples) * 100
        value_vars = cat_perc_cols + ['percentage_containing_crypto']
        df_melted = pd.melt(self.size_comparison.reset_index(), id_vars=['year'], value_vars=value_vars)

        ax = sns.lineplot(x='year', y='value', hue='variable', data=df_melted, marker='o')
        ax.set(ylim=(0, 100))
        ax.xaxis.set_major_locator(matplotlib.ticker.MultipleLocator(1))
        ax.xaxis.set_major_formatter(matplotlib.ticker.ScalarFormatter())
        ax.yaxis.set_major_formatter(matplotlib.ticker.PercentFormatter())
        ax.set(xlabel='Year', ylabel='% of sample with API',
               title='Percentage of samples containing specific cryptographic API')
        ax.figure.savefig(os.path.join(self.report_path, 'temporal_evolution.png'), dpi=300, format='png', bbox_inches='tight', pad_inches=0.1)
        ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
        plt.close()

        # Normalize absolute counts
        norm_cat_cols = ['norm_' + x for x in cat_cols]
        for norm_cat, cat in zip(norm_cat_cols, cat_cols):
            self.size_comparison[norm_cat] = self.size_comparison[cat] / self.size_comparison.index.map(self.norm_factor)

        df_melted = pd.melt(self.size_comparison.reset_index(), id_vars=['year'], value_vars=norm_cat_cols)
        df_melted = df_melted.sort_values(by='value', ascending=False)
        ax = sns.boxplot(x='variable', y='value', data=df_melted)
        plt.xticks(rotation=90)
        ax.set(xlabel='Crypto API category', ylabel='Number of ocurrences',
               title='Number of ocurrences over years per 10k samples')
        ax.figure.savefig(os.path.join(self.report_path, 'boxplot.png'), dpi=300, format='png',
                          bbox_inches='tight', pad_inches=0.1)
        ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
        plt.close()

        return df_melted

    def plot_all_categories(self):
        pass

    def plot_all_subcategories(self):
        for category in self.api_definition.keys():
            for subcategory in self.api_definition[category]:
                if subcategory != 'obfuscated':
                    self.plot_subcategory(category, subcategory)

    def plot_subcategory(self, category_name, subcategory_name, drop_treshold=20):
        cat_path = os.path.join(self.report_path, category_name)
        sub_cat_path = os.path.join(cat_path, subcategory_name)
        boxplot_filepath = os.path.join(sub_cat_path, 'boxplot.png')
        lineplot_filepath = os.path.join(sub_cat_path, 'lineplot.png')
        html_filepath = os.path.join(sub_cat_path, 'normalized_temporal_evolution.html')

        if not os.path.exists(cat_path):
            os.mkdir(cat_path)

        if not os.path.exists(sub_cat_path):
            os.mkdir(sub_cat_path)

        obfuscated = self.api_definition[category_name]['obfuscated']
        first_run = True
        category_df = None
        col_names = []

        for cat in self.api_definition[category_name][subcategory_name]:
            col_name = cat.split(obfuscated + '"')[1]
            col_names.append(col_name)
            df = self.df_records[self.df_records.trigger == cat]
            df = df.groupby('year').sha256.nunique().to_frame(name=col_name)
            df[col_name] = (df[col_name] / df.index.map(self.norm_factor))
            if first_run:
                category_df = df
                first_run = False
            else:
                category_df = category_df.join(df, how='outer')

        category_df = category_df.fillna(0).applymap(math.ceil)

        with open(html_filepath, 'w') as f:
            f.write(constants.HTML_STRING.format(table=category_df.to_html(classes=['pure-table',  'pure-table-horizontal'])))

        for c in category_df.columns:
            if category_df[c].sum() < drop_treshold:
                category_df.drop(columns=c, inplace=True)
                col_names.remove(c)

        if category_df.empty:
            return

        # Draw lineplot
        category_melted = pd.melt(category_df.reset_index(), id_vars=['year'], value_vars=col_names).sort_values(
            by='value', ascending=False)
        ax = sns.lineplot(x='year', y='value', hue='variable', data=category_melted, marker='o')
        ax.set(xlabel='Year', ylabel='Normalized frequency per 10k samples',
               title=f'Subcategory: {category_name}: {subcategory_name}, values with n>{drop_treshold}')
        ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
        ax.figure.savefig(lineplot_filepath, dpi=300, format='png',
                          bbox_inches='tight', pad_inches=0.1)
        plt.close()

        # Draw boxplot
        ax = sns.boxplot(x='variable', y='value', data=category_melted)
        plt.xticks(rotation=90)
        ax.set(xlabel=f'{subcategory_name}', ylabel='Normalized frequency per 10k samples',
               title=f'Distribution of {category_name}: {subcategory_name}, values with n>{drop_treshold}')
        ax.figure.savefig(boxplot_filepath, dpi=300, format='png',
                          bbox_inches='tight', pad_inches=0.1)
        plt.close()

    @staticmethod
    def plot_imports_comparison(df_imports, filepath, norm_factor):
        import_frequency = helpers.get_col_values_grouped_by_year_normalized(df_imports, 'api_class', 'import_frequency', norm_factor)
        import_frequency = import_frequency[import_frequency['normalized_import_frequency'] > 100]

        ax = sns.catplot(x='api_class', y='normalized_import_frequency', hue='year', data=import_frequency, kind='box', aspect=3)
        ax.set_xticklabels(rotation=90)
        ax.set(xlabel='java API class name',
               ylabel='Number of imports per 10k samples',
               title='Most popular crypto API classes')
        plt.savefig(filepath, dpi=300, format='png', bbox_inches='tight', pad_inches=0.1)
        plt.close()

    @staticmethod
    def plot_euphony(df, outpath):
        def plot_euphony_single(col_name, title_prefix, xlabel):
            euphony_series = df.dropna(subset=[col_name]).groupby('year')[col_name].value_counts()

            years = list(euphony_series.index.levels[0])
            norm_factor = {year: (euphony_series[year].sum() / 10000) for year in years}
            subset_sizes = {key: int(val * 10000) for key, val in norm_factor.items()}

            euphony_df = pd.DataFrame(euphony_series).rename(columns={col_name: 'count'}).reset_index()
            euphony_df['count'] = euphony_df['count'].astype('float64')
            euphony_df['normalized_count'] = euphony_df['count'] / euphony_df['year'].map(norm_factor)
            euphony_df['normalized_count'] = euphony_df['normalized_count'].fillna(0).astype('int64')
            euphony_df = euphony_df.sort_values(by=['normalized_count'], ascending=False).groupby('year').head(top_n_to_take)

            with open(os.path.join(outpath, col_name + 's.html'), 'w') as f:
                f.write(constants.HTML_STRING.format(table=euphony_df.to_html(classes='mystyle')))

            ax = sns.catplot(x=col_name, y='normalized_count', hue='year', data=euphony_df, kind='bar', aspect=3)
            ax.set_xticklabels(rotation=90)
            ax.set(xlabel=xlabel, ylabel='Number of samples per 10k',
                   title=f'Top {top_n_to_take} {title_prefix} for each year. Labeled subset sizes: {subset_sizes}')

            filepath = os.path.join(outpath, title_prefix + '.png')
            plt.savefig(filepath, dpi=300, format='png', bbox_inches='tight', pad_inches=0.1)
            plt.close()

        top_n_to_take = 10
        plot_euphony_single('euphony_name', 'euphony names', 'Euphony name')
        plot_euphony_single('euphony_type', 'euphony types', 'Euphony type')

    @staticmethod
    def plot_ciphers(df, outpath, cipher_triggers):
        def parse_triggers(triggers_to_parse):
            triggers_to_parse.remove('Cipher')
            triggers_to_parse.remove('DECRYPT_MODE')
            triggers_to_parse.remove('ENCRYPT_MODE')
            triggers_to_parse.remove('PRIVATE_KEY')
            triggers_to_parse.remove('PUBLIC_KEY')
            triggers_to_parse.remove('SECRET_KEY')

            return triggers_to_parse,  [escape(x) for x in triggers_to_parse], triggers_to_parse[0]

        def escape(string):
            return string.replace('(', '\(')

        def map_defaults(x):
            if str(x) == 'Cipher.getInstance("AES':
                return 'Cipher.getInstance("AES/ECB/NoPadding'
            else:
                return x

        def strip_get_instance(x):
            prefix = 'Cipher.getInstance('
            if x == prefix:
                return 'obfuscated'

            if str(x).startswith(prefix):
                return str(x)[len(prefix) + 1:]
            else:
                return x

        triggers, escaped_triggers, generic_trigger = parse_triggers(cipher_triggers)

        df_cipher = df.loc[df.trigger.str.contains('|'.join(escaped_triggers))]
        # df_cipher.trigger = df_cipher.trigger.map(map_defaults)
        # df_cipher.trigger = df_cipher.trigger.map(strip_get_instance)
        # df_cipher = df_cipher.drop_duplicates(subset=['sha256', 'class_name', 'line_number'], keep='last')
        # val_counts = df_cipher.trigger.value_counts()

        return df_cipher


def main():
    logging.basicConfig(level=logging.INFO)

    parser = argparse.ArgumentParser(description='Evaluation of the cryptographic experiments')
    parser.add_argument('exp_path', type=str, help='path to the folder where report should be placed')
    parser.add_argument('report_path', type=str, help='path to the folder with experiment folders')
    parser.add_argument('-s', '--show', type=bool, const=True, nargs='?', help='show output into stdout', default=False)
    parser.add_argument('-f', '--file', type=bool, const=True, nargs='?', help='Save files into output folders', default=False)
    args = parser.parse_args()

    start = datetime.now()

    logging.info('Starting evaluation.')
    evaluator = Evaluator(args.exp_path, args.report_path, args.show, args.file)
    evaluator.evaluate()

    end = datetime.now()
    logging.info(f'Finished experiment. Computation took {end - start} seconds.')


if __name__ == '__main__':
    main()
