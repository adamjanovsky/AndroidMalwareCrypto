import pandas as pd
import numpy as np

import os
from tqdm import tqdm
import androidcrypto.helpers as helpers
import androidcrypto.constants as constants
import base64
from time import gmtime, strftime
from importlib import reload
import logging
import neptune
import copy
import androguard
from androidcrypto.dataset import ParallelDecompiler, DatasetException
from androguard.core.analysis import auto
from shutil import copyfile
from time import gmtime, strftime
import yaml
from checksumdir import dirhash
import io
from contextlib import redirect_stdout
import re


class Task:
    """
    Partially abstract class. It covers general case of running a task that comprises of:
    1) Initializing Neptune.ml experiment and local folder with results
    2) running the experiment on each of the datasets
        - where 'running on a dataset' is an abstract function
    3) Uploading the experiment artifacts to the Neptune.ml service
    4) Terminating the experiment

    Specific cases are extended by child classes
    """
    def __init__(self, experiment_config):
        self.cfg = experiment_config
        self.experiment = None
        self.experiment_id = 'untitled'

    def init_neptune(self):
        """
        Creates new Neptune experiment and prepares folder to store results into
        """
        if self.cfg.is_being_logged:
            logging.info('Creating neptune experiment')
            neptune.init(project_qualified_name=self.cfg.neptune_project_name,
                         api_token=self.cfg.neptune_api_token)
            self.experiment = neptune.create_experiment(name=self.cfg.experiment_name)

            for dset in self.cfg.datasets:
                self.experiment.set_property(dset.name + ' version', dset.sha_digest)
                self.experiment.append_tag(dset.name)

            self.cfg.experiment_id = str(self.experiment.id)
            self._update_folder_structure(self.cfg.experiment_id)
        else:
            suffix_dirname = strftime("%Y%m%dt%H%M%S", gmtime())
            self._update_folder_structure(suffix_dirname)

    def _update_folder_structure(self, suffix_dirname):
        """
        Moves folder from temporary path to the folder with the ID of the experiment
        """
        logging.info('Updating folder structure.')
        old_logging_path = self.cfg.logging_path
        self.cfg.output_path = os.path.join(self.cfg.output_path, suffix_dirname)
        os.mkdir(self.cfg.output_path)
        self.cfg.logging_path = os.path.join(self.cfg.output_path, constants.LOGS_FILENAME)
        self._move_logging(old_logging_path, self.cfg.logging_path)

    @staticmethod
    def _move_logging(src, dst):
        """
       Moves logging file from src filepath to dst filepath (done when folder with results is established).
       :param src: destination to move from
       :param dst: destination to move to
       :return: Nothing
       """
        reload(logging)
        os.rename(src, dst)
        logging.basicConfig(filename=dst, level=logging.INFO, filemode='a',
                            format='%(asctime)s - %(levelname)s - %(funcName)s - %(message)s')

    def run(self):
        """
        Runs the experiment on all datasets
        :return:
        """
        self.init_neptune()
        for dset in self.cfg.datasets:
            self.run_on_dataset(dset)
        self.eval_neptune()

    def run_on_dataset(self, dset):
        raise NotImplementedError('Abstract method -- not meant to be implemented.')

    def eval_neptune(self):
        """
        Uploads the experiment artifacts into the Neptune.ml
        :return:
        """
        if self.cfg.is_being_logged:
            self.cfg.logger.info('Uploading experiment to Neptune.ml')
            neptune.send_artifact(self.cfg.logging_path)
            neptune.send_artifact(self.cfg.config_path)
            neptune.stop()


class DeobfuscateBase64(Task):
    """
    Task to extract, decode and upload all Base64 encoded strings from the java source-code obtained from decompiled apks.
    Follows the general structure, no comments present.
    """
    def __init__(self, experiment_config):
        super().__init__(experiment_config)

        self.jadx = pyjadx.Jadx()

    def init_neptune(self):
        super().init_neptune()
        self.experiment.append_tag('base64')

    def run_on_dataset(self, dset):
        self.cfg.logger.info(f'Analyzing base64 in {dset.name} dataset.')
        if dset.format != 'lzma':
            self.cfg.logger.info('Failed to Deobfuscate base64 strings in dataset, need lzma Analysis files, got apk format.')
            return

        results_filepath = helpers.establish_results_filepath(self.cfg.output_path, constants.BASE64_EXP_RESULTS_FILENAME + '_' + dset.name, 'txt')

        with open(results_filepath, 'w') as handle:
            counter = 0
            n_total_strings = 0
            for anobj in dset.generator():
                base64_strings = self.decode_base64_analysis_object(anobj)
                if len(base64_strings) > 0:
                    counter += 1
                    n_total_strings += len(base64_strings)

                    # TODO: Learn to get analysis object filename in a line below
                    handle.write(f'filename\n')
                    for s in base64_strings:
                        handle.write(f'\t{s}\n')
            handle.write(f'In total, {n_total_strings} Base64 strings were found in {counter} distinct files.')

    def decode_base64_analysis_object(self, anobj):
        classes = anobj.get_classes()
        base64_strings = []

        for cls in classes:
            src_class = helpers.extract_class_source_code(cls)
            if src_class is None:
                return base64_strings
            for line in src_class.split('\n'):
                if 'Base64' in line:
                    res = self.decode_base64_line_of_code(line)
                    if res is not None:
                        base64_strings.append(res)
        return base64_strings

    @staticmethod
    def decode_base64_line_of_code(line_of_code):
        splt = line_of_code.split('Base64.decode("')
        if len(splt) > 1:
            return base64.decodebytes(str.encode(splt[1].split('\"')[0]))
        else:
            return None


class ComputeStringStatistics(Task):
    """
    Task to transform apk dataset into lzma dataset.
    """
    def __init__(self, experiment_config):
        super().__init__(experiment_config)


class TransformDatasets(Task):
    def __init__(self, experiment_config):
        super().__init__(experiment_config)
        self.failed_samples = 0

    def init_neptune(self):
        super().init_neptune()
        if self.cfg.is_being_logged:
            self.experiment.append_tag('Decompile')

    def run_on_dataset(self, dset):
        new_path = os.path.join(self.cfg.output_path, dset.name)
        if not os.path.exists(new_path):
            os.makedirs(new_path)
        else:
            raise DatasetException('path already exists.')

        data_path = os.path.join(new_path, 'data/')
        os.makedirs(data_path)

        self.cfg.logger.info('Initializing decompiler class')
        decompiler = ParallelDecompiler(dset.samples, dset.root_path, new_path, self.cfg.n_threads)

        self.cfg.logger.info(f'Running decompiler on {dset.name} dataset.')

        self.cfg.logger.setLevel(logging.ERROR)  # Warning: Changing this to logging.INFO significantly slows down the code.
        self.failed_samples = decompiler.process_dataset()
        self.cfg.logger.setLevel(logging.INFO)

        self.cfg.logger.info(f'Decompilation failed on {len(self.failed_samples)} samples')
        for sample in self.failed_samples:
            self.cfg.logger.info(f'Decompilation failed on: {sample}')

        new_dset = dset.update_dset_record(new_path, 'lzma')

    def eval_neptune(self):
        super().eval_neptune()
        if self.cfg.is_being_logged is True:
            neptune.log_metric('n_failed_samples', self.failed_samples)



"""
class ComputeStringStatistics(Task):
    def __init__(self, jadx, apks_to_search, output_path, strings_to_search=None, count_imports=True):
        self.jadx = jadx
        self.strings_to_search = strings_to_search
        self.apks_to_search = apks_to_search
        self.count_imports = count_imports

        self.csv_filename = helpers.establish_results_filepath(output_path, 'results', 'csv')
        self.failed_files_filename = helpers.establish_results_filepath(output_path, 'failed_files', 'txt')

        self.occurrences_counter = pd.DataFrame(0, index=np.arange(len(self.apks_to_search)), columns=self.strings_to_search)
        self.occurrences_counter['file'] = [os.path.basename(x) for x in self.apks_to_search]
        cols = self.occurrences_counter.columns.tolist()
        cols = cols[-1:] + cols[:-1]
        self.occurrences_counter = self.occurrences_counter[cols]

    def run(self):
        correctly_processed = []

        with open(self.failed_files_filename, 'w') as f:
            for i, apk in tqdm(enumerate(self.apks_to_search)):
                if not self.count_occurrences_in_code(apk, i):
                    f.write(f'{apk}\n')
                    print(os.path.basename(apk))
                    self.occurrences_counter = self.occurrences_counter[self.occurrences_counter.file != os.path.basename(apk)]
                else:
                    correctly_processed.append(apk)

        family_names = [helpers.get_apk_label(path) for path in correctly_processed]
        sum_row = {self.occurrences_counter.columns.tolist()[0]: len(self.apks_to_search)}

        for col in self.occurrences_counter.columns.tolist()[1:]:
            sum_row[col] = self.occurrences_counter[col].sum()
        sum_row['family_name'] = 0
        sum_df = pd.DataFrame(sum_row, index=["sum"])

        # add the family name column
        self.occurrences_counter['family_name'] = family_names

        self.occurrences_counter = self.occurrences_counter.append(sum_df)
        self.occurrences_counter.to_csv(self.csv_filename, index=False, sep=';')

    def count_occurrences_in_code(self, apk, i):
        try:
            decompiler = self.jadx.load(apk)
        except:
            return False

        for cls in decompiler.classes:
            if cls.fullname.split('.')[0] == 'android':
                continue
            for line in cls.code.split('\n'):
                for s in self.strings_to_search:
                    if s not in line:
                        continue
                    if not self.count_imports and line.split(' ') and line.split(' ')[0] == 'import':
                        continue
                    self.occurrences_counter.at[i, s] += 1
        return True
"""