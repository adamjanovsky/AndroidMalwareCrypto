import csv
import itertools
import re
import hashlib
from androidcrypto import constants
import base64
import random
import logging
import json
import numpy as np
import pandas as pd
import copy
from collections.abc import Iterable

def decolorize_source_code(src_code):
    reaesc = re.compile(r'\x1b[^m]*m')
    return reaesc.sub('', src_code)


def flatten_list(lst):
    return [item for sublist in lst for item in sublist]


def hash_file(path):
    hasher = hashlib.sha256()
    with open(path, 'rb') as afile:
        buf = afile.read(constants.BLOCKSIZE)
        while len(buf) > 0:
            hasher.update(buf)
            buf = afile.read(constants.BLOCKSIZE)
    return hasher.hexdigest()


def decode_base64_line_of_code(line_of_code):
    splt = line_of_code.split('Base64.decode("')
    if len(splt) > 1:
        return base64.decodebytes(str.encode(splt[1].split('\"')[0]))
    else:
        return None


def shuffle_csv(clean_csv_path, shuffled_csv_path):
    with open(clean_csv_path, 'r') as handle:
        data = handle.readlines()
        header, rest = data[0], data[1:]
    random.shuffle(rest)
    with open(shuffled_csv_path, 'w') as handle:
        handle.write(header)
        for line in rest:
            handle.write(line)


def get_buckets(n_samples, dex_date_start, dex_date_end):
    n_buckets = dex_date_end - dex_date_start + 1
    bucket_size = n_samples // n_buckets
    remainder = n_samples % n_buckets

    return {key: bucket_size for key in range(dex_date_start, dex_date_end + 1)}, remainder


def filter_androzoo_dataset(n_samples, csv_path, dex_date_start, dex_date_end, vt_treshold, max_size, strategy):
    if strategy != constants.DOWNLOAD_STRATEGY_UNIFORM:
        return None

    samples_of_interest = []
    buckets, remainder = get_buckets(n_samples, dex_date_start, dex_date_end)

    for year, _ in zip(buckets.keys(), range(remainder, 0, -1)):
        buckets[year] += 1

    with open(csv_path, 'r') as csv_handle:
        reader = csv.DictReader(csv_handle)

        sampled = 0
        while sampled < n_samples:
            try:
                row = next(reader)
            except StopIteration:
                logging.warning(f'While you wished to get {n_samples} samples from Androzoo, the filtered dataset contains only {sampled} samples.')
                return samples_of_interest

            virus_total = int(row['vt_detection']) if row['vt_detection'] != '' else 0
            dex_date = int(row['dex_date'].split('-')[0]) if row['dex_date'].split('-')[0] != '' else 0
            file_size = int(row['apk_size'])

            if virus_total >= vt_treshold and buckets.get(dex_date, 0) > 0 and file_size < max_size:
                samples_of_interest.append(row)
                sampled += 1
                buckets[dex_date] -= 1

    return samples_of_interest

def androzoo_parse_year(dex_date):
    return int(dex_date.split('-')[0]) if dex_date.split('-')[0] != '' else 0


def get_androzoo_url(sha256, api_key):
    return 'https://androzoo.uni.lu/api/download?apikey=' + api_key + '&sha256=' + sha256


def merge_jsons(*args):
    def merge_into_globalset(new_candidates):
        new_ids = set(new_candidates.keys())
        collisions = globalset_ids.intersection(new_ids)
        globalset_ids.update(new_ids)

        if collisions:
            logging.warning(f'Collisions, introduced duplicates: {collisions}')

        # we want to keep the oldest sample, thus we must start adding from the oldest
        for sample_id in new_ids.difference(collisions):
            global_data[sample_id] = new_dict[sample_id]

    globalset_ids = set()
    global_data = {}

    for arg in args:
        with open(arg, 'r') as handle:
            new_dict = json.load(handle)
        merge_into_globalset(new_dict)

    return global_data


def introduce_missing_vals(df, cols):
    def replace_empty(x):
        if isinstance(x, Iterable):
            return x if len(x) != 0 else np.nan
        elif isinstance(x, str):
            return x if x is not None else np.nan
        elif x is None:
            return np.nan

    if not cols:
        return df

    new_df = copy.deepcopy(df)
    for c in cols:
        new_df[c] = new_df[c].apply(replace_empty)

    return new_df


def drop_adware(df):
    return df.loc[df.euphony_type != 'adware']


def print_delim(stream):
    print('-' * 80, file=stream)


def get_third_party_libs_df(df, category):
    data = []
    for index, row in enumerate(df.itertuples()):
        if isinstance(lst := getattr(row, category), list):
            data.extend([(row.Index, row.year, x) for x in lst])
    return pd.DataFrame(data, columns=['sha256', 'year', 'lib_name'])


def get_col_values_grouped_by_year_normalized(df, old_col_name, new_col_name, norm_factor, dropna=False):
    if dropna is True:
        series = df.dropna(subset=[old_col_name]).groupby('year')[old_col_name].value_counts()
    else:
        series = df.groupby('year')[old_col_name].value_counts()
    norm_name = 'normalized_' + new_col_name

    freq_df = pd.DataFrame(series).rename(columns={old_col_name: new_col_name}).reset_index()
    freq_df[new_col_name] = freq_df[new_col_name].astype('float64')
    freq_df[norm_name] = freq_df[new_col_name] / freq_df['year'].map(norm_factor)
    freq_df[norm_name] = freq_df[norm_name].astype('int64')

    return freq_df


def flatten(dct, include_obfuscated=False):
    lst = [] if not include_obfuscated else [dct['obfuscated']]
    for key in dct.keys():
        if key != 'obfuscated':
            lst.extend(dct[key])
    return lst