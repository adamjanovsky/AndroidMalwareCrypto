from androidcrypto.analyzers.task import Task
import yaml
import pandas as pd
from multiprocessing import Pool
from tqdm import tqdm
import os
import neptune
import lzma, pickle


class ExtractJavaxCrypto(Task):
    """
    Task to transform apk dataset into lzma dataset.
    """
    def __init__(self, experiment_config):
        super().__init__(experiment_config)
        self.keywords = self.identify_keywords(self.cfg.config_path)
        self.df = pd.DataFrame(columns=self.keywords)
        self.df_triggered = None
        self.triggered_data = []

        self.failed_samples = []
        self.total_failed_samples = 0
        self.total_samples = 0

    def identify_keywords(self, config_path):
        with open(config_path) as stream:
            config_stream = yaml.load(stream, Loader=yaml.FullLoader)
            return config_stream['javax_extractor']['strings_to_search']

    def init_neptune(self):
        super().init_neptune()
        if self.cfg.is_being_logged:
            self.experiment.append_tag('decompile')

    def run_on_dataset(self, dset):
        self.total_samples += self.current_dataset.n_samples
        if dset.format == 'apk':
            print('will not work man...')

        process_pool = Pool(self.cfg.n_threads)

        with tqdm(total=self.current_dataset.n_samples) as self.pbar:
            for path in dset.generator_path():
                process_pool.apply_async(self.worker_path, (path, self.keywords,), callback=self.worker_callback)

            # for smpl, path in dset.generator_tuple():
            #     process_pool.apply_async(self.worker, (smpl, path, self.keywords,), callback=self.worker_callback)
            process_pool.close()
            process_pool.join()

        if self.cfg.is_being_logged is True:
            self.update_neptune_progress()

    @staticmethod
    def worker_path(path, keywords):

        trigger_result = []
        result = [0] * len(keywords)
        with lzma.open(path, 'rb') as handle:
            sample = pickle.load(handle)
        classes = sample.get_classes()

        try:
            for cls in classes:
                src_class = cls.get_vm_class().get_source()
                if src_class is None:
                    continue
                for i, line in enumerate(src_class.split('\n')):
                    for wrd, rs in zip(keywords, result):
                        if wrd in line:
                            trigger_result.append([path, cls.name, wrd, line, i])
            return None, trigger_result
        except Exception:
            return path, None

    @staticmethod
    def worker(sample, path, keywords):
        trigger_result = []
        result = [0] * len(keywords)
        classes = sample.get_classes()
        try:
            for cls in classes:
                src_class = cls.get_vm_class().get_source()
                if src_class is None:
                    continue
                for i, line in enumerate(src_class.split('\n')):
                    for wrd, rs in zip(keywords, result):
                        if wrd in line:
                            trigger_result.append([path, cls.name, wrd, line, i])
                            break
            return None, trigger_result
        except Exception:
            return path, None

    def worker_callback(self, result):
        if result[0] is None:
            self.triggered_data.extend([smpl for smpl in result[1]])
        else:
            self.failed_samples.append(result[0])
            self.total_failed_samples += 1
        super().worker_callback(result)

    def eval_neptune(self):
        if self.cfg.is_being_logged is True:
            failed_samples_ratio = str(self.total_failed_samples) + '/' + str(self.total_samples)
            neptune.log_text('total_failed_samples', failed_samples_ratio)
        super().eval_neptune()

    def clean_up_after_dataset(self):
        self.df_triggered = pd.DataFrame(columns=['path', 'class', 'triggered_by', 'line', 'line_number'], data=self.triggered_data)
        self.df_triggered.to_csv(os.path.join(self.cfg.output_path, 'results' + self.current_dataset.name + '.csv'))

        if self.cfg.is_being_logged is True:
            failed_samples_ratio = str(len(self.failed_samples)) + '/' + str(self.current_dataset.n_samples)
            neptune.log_text(self.current_dataset.name + '_failed_samples', failed_samples_ratio)
        self.failed_samples = []
        super().clean_up_after_dataset()
