from androidcrypto.analyzers.task import Task
import os
import logging
import neptune
from androidcrypto.dataset import  DatasetException
import lzma
import pickle
from androguard import misc
from multiprocessing import Pool
from datetime import datetime
from tqdm import tqdm
import shutil

import csv

from androguard.core.bytecodes.apk import APK
from androguard.core.bytecodes.dvm import DalvikVMFormat
from androguard.core.analysis.analysis import Analysis
from androguard.decompiler.decompiler import DecompilerJADX


class TransformDatasets(Task):
    def __init__(self, experiment_config):
        super().__init__(experiment_config)
        self.failed_samples = []
        self.total_failed_samples = 0
        self.total_samples = 0
        self.current_dset_new_path = None

        self.custom_jadx_path = None
        self.prepare_jadx_binary()

        self.delete_apks = self.cfg.stream['delete_apks']

        self.home_packages = {}

    def prepare_jadx_binary(self):
        if 'jadx_path' in self.cfg.stream:
            self.custom_jadx_path = self.cfg.stream['jadx_path']
        else:
            self.custom_jadx_path = 'jadx'
        if not shutil.which(self.custom_jadx_path):
            raise Exception(f'Jadx binary not found in the path {self.custom_jadx_path}.')

    def init_neptune(self):
        super().init_neptune()
        if self.cfg.is_being_logged:
            self.experiment.append_tag('decompile')

    def run_on_dataset(self, dset):
        self.dset_start_time = datetime.now()
        self.progress_metric_name = 'progress_' + self.current_dataset.name
        self.total_samples += self.current_dataset.n_samples
        self.update_neptune_progress()
        self.current_dset_new_path = os.path.join(self.cfg.output_path, os.path.basename(dset.root_path))
        if not os.path.exists(self.current_dset_new_path):
            os.makedirs(self.current_dset_new_path)
        else:
            raise DatasetException('path already exists.')

        data_path = os.path.join(self.current_dset_new_path, 'data/')
        os.makedirs(data_path)

        self.cfg.logger.info('Initializing decompiler class')
        self.cfg.logger.info(f'Running decompiler on {dset.name} dataset.')
        self.cfg.logger.setLevel(logging.ERROR)  # Warning: Changing this to logging.INFO significantly slows down the code.

        process_pool = Pool(self.cfg.n_threads)
        with tqdm(total=self.current_dataset.n_samples) as self.pbar:
            for smpl in dset.generator():
                process_pool.apply_async(self.worker_jadx, (smpl, self.current_dataset.root_path, self.current_dset_new_path, self.custom_jadx_path, self.delete_apks, ), callback=self.worker_callback)
            process_pool.close()
            process_pool.join()

        self.cfg.logger.setLevel(logging.INFO)
        self.cfg.logger.info(f'Decompilation failed on {len(self.failed_samples)} samples')
        for sample in self.failed_samples:
            self.cfg.logger.info(f'Decompilation failed on: {sample}')

        new_dset = dset.update_dset_record(self.current_dset_new_path, 'lzma')

        with open(os.path.join(self.current_dset_new_path + '_packages.csv'), 'w') as f:
            for key in self.home_packages.keys():
                f.write("%s, %s\n" % (key, self.home_packages[key]))

        self.update_neptune_progress()

    @staticmethod
    def worker(sample, curr_root_path, new_root_path):
        """
        Present for legacy reasons, does not use jadx.
        """
        relative_dir = os.path.dirname(os.path.relpath(sample, curr_root_path))
        new_dirname = os.path.join(new_root_path, relative_dir)
        new_basename = str(os.path.basename(sample).split('apk')[0]) + 'lzma'
        new_path = os.path.join(new_dirname, new_basename)

        try:
            a, d, analysis_object = misc.AnalyzeAPK(sample)

            if not os.path.exists(new_dirname):
                os.makedirs(new_dirname)

            if len(analysis_object.classes) > 0:
                with lzma.open(new_path, 'wb') as handle:
                    pickle.dump(analysis_object, handle)
            return None

        except Exception:
            if new_path is not None and os.path.exists(new_path):
                os.remove(new_path)
            return os.path.join(relative_dir, os.path.basename(sample))

    @staticmethod
    def worker_jadx(sample, curr_root_path, new_root_path, custom_jadx_path, delete_apks):
        """
        Jadx decompiler works better than default DED. Thus, we recommend to work with JADX. However, be careful as
        Androguard has a bug that prevents Jadx to be used out of the box. Until Androguard bug is fixed, deploy your
        own version with this fix: https://github.com/androguard/androguard/issues/763

        Note that Jadx doesn't work so smoothly with Androguard which in turn costs probably 50% of performance.

        Will not work on multiple dex files present in the apk. In such case, major fix is needed within the classed.dex
        """
        relative_dir = os.path.dirname(os.path.relpath(sample, curr_root_path))
        new_dirname = os.path.join(new_root_path, relative_dir)
        new_basename = str(os.path.basename(sample).split('apk')[0]) + 'lzma'
        new_path = os.path.join(new_dirname, new_basename)

        try:
            a = APK(sample)
            pkg = a.get_package()

            if delete_apks is True:
                os.remove(sample)

            d = DalvikVMFormat(a)
            dx = Analysis(d)
            d.set_decompiler(DecompilerJADX(d, dx, jadx=custom_jadx_path))

            if not os.path.exists(new_dirname):
                os.makedirs(new_dirname)

            if len(dx.classes) > 0:
                with lzma.open(new_path, 'wb') as handle:
                    pickle.dump(dx, handle)
            return os.path.basename(sample).split('.')[0], pkg, None

        except Exception:
            if new_path is not None and os.path.exists(new_path):
                os.remove(new_path)
            return None, None, os.path.join(relative_dir, os.path.basename(sample))

    def worker_callback(self, result):
        if result[0] is None:
            self.failed_samples.append(result[2])
            self.total_failed_samples += 1
        else:
            self.home_packages[result[0]] = result[1]

        super().worker_callback(result)

    def eval_neptune(self):
        if self.cfg.is_being_logged is True:
            failed_samples_ratio = str(self.total_failed_samples) + '/' + str(self.total_samples)
            neptune.log_text('total_failed_samples', failed_samples_ratio)
        super().eval_neptune()

    def clean_up_after_dataset(self):
        if self.cfg.is_being_logged is True:
            failed_samples_ratio = str(len(self.failed_samples)) + '/' + str(self.current_dataset.n_samples)
            neptune.log_text(self.current_dataset.name + '_failed_samples', failed_samples_ratio)
        self.failed_samples = []
        super().clean_up_after_dataset()
