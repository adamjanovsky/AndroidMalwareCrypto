from androidcrypto.analyzers.task import Task
import os
import logging
import neptune
from androidcrypto.dataset import  DatasetException
import lzma
import pickle
from androguard import misc
from multiprocessing import Pool
from datetime import datetime
from tqdm import tqdm


class TransformDatasets(Task):
    def __init__(self, experiment_config):
        super().__init__(experiment_config)
        self.failed_samples = []
        self.total_failed_samples = 0
        self.current_dset_new_path = None

    def init_neptune(self):
        super().init_neptune()
        if self.cfg.is_being_logged:
            self.experiment.append_tag('Decompile')

    def run_on_dataset(self, dset):
        self.dset_start_time = datetime.now()
        self.update_neptune_progress()
        self.current_dset_new_path = os.path.join(self.cfg.output_path, os.path.basename(dset.root_path))
        if not os.path.exists(self.current_dset_new_path):
            os.makedirs(self.current_dset_new_path)
        else:
            raise DatasetException('path already exists.')

        data_path = os.path.join(self.current_dset_new_path, 'data/')
        os.makedirs(data_path)

        self.cfg.logger.info('Initializing decompiler class')
        self.cfg.logger.info(f'Running decompiler on {dset.name} dataset.')
        self.cfg.logger.setLevel(logging.ERROR)  # Warning: Changing this to logging.INFO significantly slows down the code.

        process_pool = Pool(self.cfg.n_threads)
        with tqdm(total=self.current_dataset.n_samples) as self.pbar:
            for smpl in dset.generator():
                process_pool.apply_async(self.worker, (smpl, self.current_dataset.root_path, self.current_dset_new_path,), callback=self.worker_callback)
            process_pool.close()
            process_pool.join()

        self.cfg.logger.setLevel(logging.INFO)
        self.cfg.logger.info(f'Decompilation failed on {len(self.failed_samples)} samples')
        for sample in self.failed_samples:
            self.cfg.logger.info(f'Decompilation failed on: {sample}')

        new_dset = dset.update_dset_record(self.current_dset_new_path, 'lzma')

        self.update_neptune_progress()
        self.clean_up_after_dataset()

    @staticmethod
    def worker(sample, curr_root_path, new_root_path):
        relative_dir = os.path.dirname(os.path.relpath(sample, curr_root_path))
        new_dirname = os.path.join(new_root_path, relative_dir)
        new_basename = str(os.path.basename(sample).split('apk')[0]) + 'lzma'
        new_path = os.path.join(new_dirname, new_basename)

        try:
            a, d, analysis_object = misc.AnalyzeAPK(sample)

            if not os.path.exists(new_dirname):
                os.makedirs(new_dirname)

            if len(analysis_object.classes) > 0:
                with lzma.open(new_path, 'wb') as handle:
                    pickle.dump(analysis_object, handle)
            return None

        except Exception:
            if new_path is not None and os.path.exists(new_path):
                os.remove(new_path)
            return os.path.join(relative_dir, os.path.basename(sample))

    def worker_callback(self, result):
        if result is not None:
            self.failed_samples.append(result)
            self.total_failed_samples += 1

        super().worker_callback(result)

    def eval_neptune(self):
        if self.cfg.is_being_logged is True:
            neptune.log_metric('n_failed_samples', self.total_failed_samples)
        super().eval_neptune()

    def clean_up_after_dataset(self):
<<<<<<< Updated upstream
=======
        if self.cfg.is_being_logged is True:
            metric_name = self.current_dataset.name + '_failed_samples'
            neptune.log_metric(metric_name, len(self.failed_samples))
>>>>>>> Stashed changes
        self.failed_samples = []
        self.processed_samples = 0
