import androidcrypto.constants as constants
from androguard.core.analysis import auto
from checksumdir import dirhash
from hashlib import sha256
import lzma
import pickle
import os
import logging
import yaml
from os.path import *
import lzma
from tqdm import *

logging.getLogger('requests').setLevel(logging.DEBUG)

from androguard.core.analysis import analysis
from androguard import misc
from copy import deepcopy
from shutil import copyfile

from multiprocessing.dummy import Pool as ThreadPool

class DatasetException(Exception):
    pass


class Dataset:
    """
    A dataset is expected to be stored in a folder with the following structure:
    root
    |- meta.yml (metafile containing information about the dataset
    |- Data (arbitrary subfolders with apks)
    ---| - apk1
    ---| - Subfolders
    If no sha-digest is set in meta.yml, new is computed from the Data folder.
    It is however assumed that once hash is computed, it is not being changed - we should rethink this
    TODO: Rethink the hash paradigm above
    """
    def __init__(self, root_path):
        self.root_path = root_path
        self.meta_path = None
        self.data_path = None
        self.deduce_paths()

        self.sha_digest = None
        self.n_samples = 0
        self.name = None
        self.description = None
        self.format = None

        self.samples = []

        self.config = None
        self.parse_meta()

    def deduce_paths(self):
        self.meta_path = os.path.join(self.root_path, constants.META_FILENAME)
        self.data_path = os.path.join(self.root_path, constants.DATA_DIRNAME)

    def generator(self):
        """
        Use this to iterate over dataset. Returns the Analysis object if successful, None otherwise.
        """
        for sample_path in self.samples:
            if self.format == 'apk':
                yield self.load_apk(sample_path)
            elif self.format == 'lzma':
                yield self.load_lzma(sample_path)

    @staticmethod
    def load_apk(path):
        """
        Decompile path to the apk file into Analysis object.
        """
        try:
            a, d, dx = misc.AnalyzeAPK(path)
            return dx
        except Exception:
            return None

    @staticmethod
    def load_lzma(path):
        """
        Decompress, deserialize the path to the Analysis object.
        """
        try:
            with lzma.open(path, 'rb') as handle:
                return pickle.load(handle)
        except Exception as e:
            return None

    def parse_meta(self):
        """
        Parses the meta.yml dataset file and constructs a Dataset object out of it.
        :return:
        """
        if not os.path.isfile(self.meta_path):
            raise DatasetException(f'Dataset meta file not found on the path {self.meta_path}')

        with open(self.meta_path, 'r') as meta_file:
            self.config = yaml.load(meta_file, Loader=yaml.FullLoader)
            self.name = self.config['dataset_name']
            self.description = self.config['description']
            self.format = self.config['format']

            self.sha_digest = None if self.config['sha_256'] == 'None' else self.config['sha_256']
            if self.sha_digest is None:
                self.sha_digest = dirhash(self.data_path, 'sha256')
                self.config['sha_256'] = self.sha_digest

            self.n_samples = self.config['n_samples']

            to_update = False
            if self.n_samples == 0:
                to_update = True
            for root, dir, files in os.walk(self.data_path):
                dir_apks = [os.path.join(root, f) for f in files if f.endswith(self.format)]
                if to_update:
                    self.n_samples += len(dir_apks)
                self.samples.extend(dir_apks)
            self.config['n_samples'] = self.n_samples

        self.dump_config()

    def dump_config(self):
        with open(self.meta_path, 'w') as meta_file:
            yaml.dump(self.config, meta_file)

    # TODO: Add method for simple dataset update

    def update_dset_record(self, new_root_path, new_format):
        """
        Finalizes dataset that originated from self by decompiling process.
        1) Sets new paths
        2) Updates number of samples, list of samples
        3) Updates hash
        4) Creates new meta.yml
        :return: new dataset object
        """
        new_dset = deepcopy(self)
        new_dset.root_path = new_root_path
        new_dset.deduce_paths()
        copyfile(self.meta_path, new_dset.meta_path)

        new_dset.update_format(new_format)
        new_dset.update_samples()
        new_dset.update_hash()

        new_dset.dump_config()

        return new_dset

    def update_samples(self):
        self.samples = []
        self.n_samples = 0
        for root, dir, files in os.walk(self.data_path):
            dir_samples = [os.path.join(root, f) for f in files if f.endswith(self.format)]
            self.n_samples += len(dir_samples)
            self.samples.extend(dir_samples)

        self.config['n_samples'] = self.n_samples

    def update_format(self, new_format):
        self.format = new_format
        self.config['format'] = new_format

    def update_hash(self):
        self.sha_digest = dirhash(self.data_path, 'sha256')
        self.config['sha_256'] = self.sha_digest


class ParallelDecompiler:
    """
    Parallel decompiler. Stars n_threads threads and decompiles the files into lzma compressed analysis objects
    """
    def __init__(self, samples, base_input_path, output_path, n_threads):
        self.samples = samples
        self.input_path = base_input_path
        self.output_path = output_path
        self.n_threads = n_threads
        self.failed_samples = []

    def worker(self, path_to_apk):
        relative_dir = dirname(relpath(path_to_apk, self.input_path))
        new_dirname = join(self.output_path, relative_dir)
        new_basename = str(basename(path_to_apk).split('.apk')[0]) + '.lzma'
        new_path = join(new_dirname, new_basename)
        try:
            a, d, analysis_object = misc.AnalyzeAPK(path_to_apk)

            if not os.path.exists(new_dirname):
                os.makedirs(new_dirname)

            if len(analysis_object.classes) > 0:
                with lzma.open(new_path, 'wb') as handle:
                    pickle.dump(analysis_object, handle)
        except Exception:
            self.failed_samples.append(os.path.join(relative_dir, basename(path_to_apk)))
            if new_path is not None and os.path.exists(new_path):
                os.remove(new_path)
            return

    def process_dataset(self):
        pool = ThreadPool(self.n_threads)

        # According to stack overflow, has to be started twice
        with tqdm(total=len(self.samples)) as progress_bar:
            for i, _ in tqdm(enumerate(pool.imap_unordered(self.worker, self.samples)), total=len(self.samples), position=0, leave=True):
                progress_bar.update()

                #pool.map(self.worker, self.samples)
        return self.failed_samples
