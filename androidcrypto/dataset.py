from androidcrypto import helpers, constants
from checksumdir import dirhash
import pickle
import os
import lzma
from androguard import misc
from copy import deepcopy
from shutil import copyfile
import yaml
import json
from copy import deepcopy


class DatasetException(Exception):
    pass

# TODO: Add hash functionality, temporarily disabled
# TODO: Add deleting corrupted samples, e.g. samples where json is available but apk / dx is not


class Dataset:
    """

    """
    def __init__(self, root_path):
        self.root_path = root_path
        self.meta_path = None
        self.json_path = None
        self.data_path = None
        self.apk_path = None
        self.dx_path = None
        self.json_data = {}
        self.apk_available = False
        self.dx_available = False
        self.sha_digest = 'So far not implemented'
        self.n_samples = 0
        self.name = None
        self.description = None
        self.format = None

        self.deduce_paths()
        self.parse_meta()

    def deduce_paths(self):
        self.meta_path = os.path.join(self.root_path, constants.META_FILENAME)
        self.data_path = os.path.join(self.root_path, constants.DATA_DIRNAME)
        self.json_path = os.path.join(self.root_path, constants.JSON_FILENAME)
        self.dx_path = os.path.join(self.data_path, constants.DX_DIRNAME)
        self.apk_path = os.path.join(self.data_path, constants.APK_DIRNAME)

    def generator_id_apk_dx(self):
        for key, val in self.json_data.items():
            yield key, val['metadata']['apk_path'], val['metadata']['dx_path']

    def generator_dx(self):
        if not self.dx_available:
            raise DatasetException('No analysis objects available.')
        for record in self.json_data:
            dx_path = record['metadata']['dx_path']
            yield self.load_lzma(dx_path)

    def generator_json(self):
        for record, val in self.json_data.items():
            yield record, val

    def generator_dx_path(self):
        if not self.dx_available:
            raise Exception('No analysis objects available.')
        for record in self.json_data:
            yield record['metadata']['dx_path']

    @staticmethod
    def load_lzma(path):
        """
        Decompress, deserialize the path to the Analysis object.
        """
        try:
            with lzma.open(path, 'rb') as handle:
                return pickle.load(handle)
        except Exception as e:
            return None

    # TODO: Refactor
    def parse_meta(self):
        """
        Parses the meta.yml dataset file and constructs a Dataset object out of it.
        :return:
        """
        if not os.path.isfile(self.meta_path):
            raise DatasetException(f'Dataset meta file not found on the path {self.meta_path}')

        with open(self.meta_path, 'r') as meta_file:
            config = yaml.load(meta_file, Loader=yaml.FullLoader)
            self.name = config['dataset_name']
            self.description = config['description']
            self.apk_available = config['apk_available']
            self.dx_available = config['dx_available']
            self.n_samples = config['n_samples']

            if os.path.isfile(self.json_path):
                with open(self.json_path, 'r') as json_file:
                    self.json_data = json.load(json_file)
            else:
                dct = {'metadata': {'apk_path': None, 'dx_path': None}}
                for root, dir, files in os.walk(self.apk_path):
                    dir_apks = [os.path.join(root, f) for f in files if f.endswith('apk')]

                    for apk in dir_apks:
                        sha1 = helpers.hash_file(apk)
                        data = deepcopy(dct)
                        data['metadata']['apk_path'] = apk
                        self.json_data[sha1] = data
                self.n_samples = len(self.json_data.keys())

                with open(self.json_path, 'w') as handle:
                    json.dump(self.json_data, handle)

        self.update_meta()

    def update_meta(self):
        with open(self.meta_path, 'w') as meta_file:
            config = {'dataset_name': self.name,
                      'description': self.description,
                      'n_samples': self.n_samples,
                      'apk_available': self.apk_available,
                      'dx_available': self.dx_available}
            yaml.dump(config, meta_file)
        with open(self.json_path, 'w') as handle:
            json.dump(self.json_data, handle)

    def update_hash(self):
        pass
        # self.sha_digest = dirhash(self.data_path, 'sha256')
        # self.config['sha_256'] = self.sha_digest

    def update_after_decompilation(self):
        # TODO: Implement a check that number of apks, dxs and samples matches the expectation, return error if not
        self.n_samples = len(self.json_data)
        self.dx_available = True
        self.update_meta()

    def delete_record(self, sample_id):
        self.delete_apk(sample_id)
        self.delete_dx(sample_id)

        try:
            self.json_data.pop(sample_id)
        except KeyError:
            pass

    def delete_apk(self, sample_id):
        # TODO: Should check if last apk in nested subfolder. If yes, delete as well.
        try:
            os.remove(self.json_data[sample_id]['metadata']['apk_path'])
        except OSError:
            pass

    def delete_dx(self, sample_id):
        # TODO: Should check if last apk in nested subfolder. If yes, delete as well.
        try:
            os.remove(self.json_data[sample_id]['metadata']['dx_path'])
        except OSError:
            pass

    def set_dx_path(self, sample_id, path):
        self.json_data[sample_id]['metadata']['dx_path'] = path