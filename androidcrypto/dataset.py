from androidcrypto import helpers, constants
from checksumdir import dirhash
import pickle
import os
import lzma
from androguard import misc
from copy import deepcopy
from shutil import copyfile
import yaml
import json
from copy import deepcopy
import csv
import tempfile, shutil
import sys


class DatasetException(Exception):
    pass

# TODO: Add hash functionality, temporarily disabled
# TODO: Add deleting corrupted samples, e.g. samples where json is available but apk / dx is not


class Dataset:
    def __init__(self, root_path):
        self.root_path = root_path
        self.meta_path = None
        self.json_path = None
        self.data_path = None
        self.apk_path = None
        self.dx_path = None
        self.third_party_tmp_path = None
        self.json_data = {}
        self.apk_available = False
        self.dx_available = False
        self.sha_digest = 'So far not implemented'
        self.n_samples = 0
        self.name = None
        self.description = None
        self.format = None
        self.androzoo_config = None
        self.failed_samples = None

        self.deduce_paths()
        self.parse_meta()

    def deduce_paths(self):
        self.meta_path = os.path.join(self.root_path, constants.META_FILENAME)
        self.data_path = os.path.join(self.root_path, constants.DATA_DIRNAME)
        self.json_path = os.path.join(self.root_path, constants.JSON_FILENAME)
        self.dx_path = os.path.join(self.data_path, constants.DX_DIRNAME)
        self.apk_path = os.path.join(self.data_path, constants.APK_DIRNAME)

    def generator_dx(self):
        if not self.dx_available:
            raise DatasetException('No analysis objects available.')
        for record in self.json_data:
            dx_path = record['metadata']['dx_path']
            yield self.load_lzma(dx_path)

    def generator_json(self):
        for key, val in self.json_data.items():
            yield key, val

    def generator_dx_path(self):
        if not self.dx_available:
            raise Exception('No analysis objects available.')
        for record in self.json_data:
            yield record['metadata']['dx_path']

    @staticmethod
    def load_lzma(path):
        """
        Decompress, deserialize the path to the Analysis object.
        """
        try:
            with lzma.open(path, 'rb') as handle:
                return pickle.load(handle)
        except Exception as e:
            return None

    # TODO: Refactor
    def parse_meta(self):
        """
        Parses the meta.yml dataset file and constructs a Dataset object out of it.
        :return:
        """
        if not os.path.isfile(self.meta_path):
            self.name = 'Pick your dataset name here'
            self.description = 'Put your dataset description here'
            self.apk_available = False
            self.dx_available = False
            self.n_samples = 0

            if not os.path.exists(self.root_path):
                os.makedirs(self.root_path)

        else:
            with open(self.meta_path, 'r') as meta_file:
                config = yaml.load(meta_file, Loader=yaml.FullLoader)
                self.name = config['dataset_name']
                self.description = config['description']
                self.apk_available = config['apk_available']
                self.dx_available = config['dx_available']
                self.n_samples = config['n_samples']

        if os.path.isfile(self.json_path):
            with open(self.json_path, 'r') as json_file:
                self.json_data = json.load(json_file)
        else:
            for root, dir, files in os.walk(self.apk_path):
                dir_apks = [os.path.join(root, f) for f in files if f.endswith('apk')]

                for apk in dir_apks:
                    sha = helpers.hash_file(apk)
                    data = deepcopy(constants.DATASET_BASE_DICT)
                    data['metadata']['apk_path'] = apk
                    self.json_data[sha] = data
            self.n_samples = len(self.json_data.keys())

        self.update_meta()

    def update_meta(self):
        with open(self.meta_path, 'w') as meta_file:
            config = {'dataset_name': self.name,
                      'description': self.description,
                      'n_samples': self.n_samples,
                      'apk_available': self.apk_available,
                      'dx_available': self.dx_available}
            yaml.dump(config, meta_file)
        with open(self.json_path, 'w') as handle:
            json.dump(self.json_data, handle)

    def update_record_file(self):
        with open(self.json_path, 'w') as handle:
            json.dump(self.json_data, handle)

    def update_hash(self):
        pass
        # self.sha_digest = dirhash(self.data_path, 'sha256')
        # self.config['sha_256'] = self.sha_digest

    def update_record(self, sample_id, new_record):
        self.json_data[sample_id] = new_record

    def delete_record(self, sample_id):
        self.delete_apk(sample_id)
        self.delete_dx(sample_id)

        try:
            self.json_data.pop(sample_id)
        except KeyError:
            pass

    def delete_apk(self, sample_id):
        # TODO: Should check if last apk in nested subfolder. If yes, delete as well.
        try:
            os.remove(self.json_data[sample_id]['metadata']['apk_path'])
        except OSError:
            pass

    def delete_dx(self, sample_id):
        # TODO: Should check if last apk in nested subfolder. If yes, delete as well.
        dx_path = self.json_data[sample_id]['metadata']['dx_path']
        try:
            if dx_path is not None:
                os.remove(dx_path)
        except OSError:
            pass

    def set_apk_path(self, sample_id, path):
        self.json_data[sample_id]['metadata']['apk_path'] = path

    def set_dx_path(self, sample_id, path):
        self.json_data[sample_id]['metadata']['dx_path'] = path

    def prepare_apk_paths(self):
        os.makedirs(self.apk_path, exist_ok=True)
        for sample_id, json_record in self.generator_json():
            apk_path = os.path.join(self.apk_path, sample_id + '.apk')
            self.set_apk_path(sample_id, apk_path)

    def prepare_dx_paths(self):
        for sample_id, json_record in self.generator_json():
            apk_path = json_record['metadata']['apk_path']
            relative_directory = os.path.dirname(os.path.relpath(apk_path, self.apk_path))
            dir_to_create = os.path.join(self.dx_path, relative_directory)
            os.makedirs(dir_to_create, exist_ok=True)
            new_dx_path = os.path.join(dir_to_create, os.path.basename(apk_path).split('.apk')[0] + '.lzma')
            self.set_dx_path(sample_id, new_dx_path)

        self.update_meta()

    def prepare_download(self, download_config):
        samples_to_download = self.filter_androzoo_dataset(download_config.n_samples, download_config.csv_path,
                                                           download_config.start_year, download_config.end_year,
                                                           download_config.minimal_vt_positives, download_config.max_size,
                                                           download_config.strategy)
        for sample in samples_to_download:
            download_url = self.get_androzoo_url(sample['sha256'], download_config.api_token)
            self.json_data[sample['sha256']] = deepcopy(constants.DATASET_BASE_DICT)
            self.json_data[sample['sha256']]['metadata']['androzoo_url'] = download_url
            self.json_data[sample['sha256']]['metadata']['vt_scan_year'] = self.androzoo_parse_year(sample['vt_scan_date'])

        self.n_samples = len(samples_to_download)


    @staticmethod
    def filter_androzoo_dataset(n_samples, csv_path, vt_scan_date_start, vt_scan_date_end, vt_treshold, max_size, strategy):
        samples_of_interest = []

        if strategy == constants.DOWNLOAD_STRATEGY_UNIFORM:
            n_buckets = vt_scan_date_end - vt_scan_date_start + 1
            bucket_size = n_samples // n_buckets
            remainder = n_samples % n_buckets

            buckets = {key: bucket_size for key in range(vt_scan_date_start, vt_scan_date_end + 1)}
            for year in buckets.keys():
                if remainder > 0:
                    buckets[year] += 1
                    remainder -= 1
                else:
                    break

        else:
            print('Not implemented yet, exiting.')
            sys.exit()

        with open(csv_path, 'r') as csv_handle:
            reader = csv.DictReader(csv_handle)
            sampled = 0

            for row in reader:
                if row['vt_detection'] != '':
                    virus_total = int(row['vt_detection'])
                else:
                    virus_total = 0

                vt_scan_year = row['vt_scan_date'].split('-')[0]
                if vt_scan_year != '':
                    vt_scan_year = int(vt_scan_year)
                else:
                    vt_scan_year = 0

                file_size = int(row['apk_size'])

                if virus_total >= vt_treshold and buckets.get(vt_scan_year, 0) > 0 and file_size < max_size:
                    samples_of_interest.append(row)
                    sampled += 1
                    buckets[vt_scan_year] -= 1

                if sampled >= n_samples:
                    return samples_of_interest

        return samples_of_interest

    @staticmethod
    def androzoo_parse_year(vt_scan_date):
        return int(vt_scan_date.split('-')[0]) if vt_scan_date.split('-')[0] != '' else 0


    @staticmethod
    def get_androzoo_url(sha256, api_key):
        return 'https://androzoo.uni.lu/api/download?apikey=' + api_key + '&sha256=' + sha256

    def prepare_third_party_lib_paths(self):
        self.third_party_tmp_path = tempfile.mkdtemp()
        for sample_id, json_record in self.generator_json():
            third_party_tmp_path = os.path.join(self.third_party_tmp_path, str(sample_id) + '.json')
            json_record['metadata']['third_party_tmp_path'] = third_party_tmp_path

    def delete_third_party_tmp(self):
        shutil.rmtree(self.third_party_tmp_path)
