from androidcrypto import helpers, constants
from checksumdir import dirhash
import pickle
import os
import lzma
from androguard import misc
from copy import deepcopy
from shutil import copyfile
import yaml


class DatasetException(Exception):
    pass


class Dataset:
    """
    A dataset is expected to be stored in a folder with the following structure:
    root
    |- meta.yml (metafile containing information about the dataset
    |- Data (arbitrary subfolders with apks)
    ---| - apk1
    ---| - Subfolders
    If no sha-digest is set in meta.yml, new is computed from the Data folder.
    It is however assumed that once hash is computed, it is not being changed - we should rethink this
    TODO: Rethink the hash paradigm above
    """
    def __init__(self, root_path):
        self.root_path = root_path
        self.meta_path = None
        self.data_path = None
        self.deduce_paths()

        self.sha_digest = None
        self.n_samples = 0
        self.name = None
        self.description = None
        self.format = None

        self.samples = []

        self.config = None
        self.parse_meta()

    def deduce_paths(self):
        self.meta_path = os.path.join(self.root_path, constants.META_FILENAME)
        self.data_path = os.path.join(self.root_path, constants.DATA_DIRNAME)

    def generator(self):
        """
        Use this to iterate over dataset. Returns the Analysis object if successful, None otherwise.
        """
        for sample_path in self.samples:
            if self.format == 'apk':
                yield sample_path
            elif self.format == 'lzma':
                yield self.load_lzma(sample_path)

    def generator_tuple(self):
        """
        :return: Returns tuple "analysis object, path".
        """
        for sample_path in self.samples:
            if self.format == 'apk':
                yield None, sample_path
            elif self.format == 'lzma':
                yield self.load_lzma(sample_path), sample_path

    def generator_path(self):
        for sample_path in self.samples:
            yield sample_path

    @staticmethod
    def load_apk(path):
        """
        Decompile path to the apk file into Analysis object.
        """
        try:
            a, d, dx = misc.AnalyzeAPK(path)
            return dx
        except Exception:
            return None

    @staticmethod
    def load_lzma(path):
        """
        Decompress, deserialize the path to the Analysis object.
        """
        try:
            with lzma.open(path, 'rb') as handle:
                return pickle.load(handle)
        except Exception as e:
            return None

    def parse_meta(self):
        """
        Parses the meta.yml dataset file and constructs a Dataset object out of it.
        :return:
        """
        if not os.path.isfile(self.meta_path):
            raise DatasetException(f'Dataset meta file not found on the path {self.meta_path}')

        with open(self.meta_path, 'r') as meta_file:
            self.config = yaml.load(meta_file, Loader=yaml.FullLoader)
            self.name = self.config['dataset_name']
            self.description = self.config['description']
            self.format = self.config['format']

            self.sha_digest = None if self.config['sha_256'] == 'None' else self.config['sha_256']
            if self.sha_digest is None:
                self.sha_digest = dirhash(self.data_path, 'sha256')
                self.config['sha_256'] = self.sha_digest

            self.n_samples = self.config['n_samples']

            to_update = False
            if self.n_samples == 0:
                to_update = True
            for root, dir, files in os.walk(self.data_path):
                dir_apks = [os.path.join(root, f) for f in files if f.endswith(self.format)]
                if to_update:
                    self.n_samples += len(dir_apks)
                self.samples.extend(dir_apks)
            self.config['n_samples'] = self.n_samples

        self.dump_config()

    def dump_config(self):
        with open(self.meta_path, 'w') as meta_file:
            yaml.dump(self.config, meta_file)

    def update_dset_record(self, new_root_path, new_format):
        """
        Finalizes dataset that originated from self by decompiling process.
        1) Sets new paths
        2) Updates number of samples, list of samples
        3) Updates hash
        4) Creates new meta.yml
        :return: new dataset object
        """
        new_dset = deepcopy(self)
        new_dset.root_path = new_root_path
        new_dset.deduce_paths()
        copyfile(self.meta_path, new_dset.meta_path)

        new_dset.update_format(new_format)
        new_dset.update_samples()
        new_dset.update_hash()

        new_dset.dump_config()

        return new_dset

    def update_samples(self):
        self.samples = []
        self.n_samples = 0
        for root, dir, files in os.walk(self.data_path):
            dir_samples = [os.path.join(root, f) for f in files if f.endswith(self.format)]
            self.n_samples += len(dir_samples)
            self.samples.extend(dir_samples)

        self.config['n_samples'] = self.n_samples

    def update_format(self, new_format):
        self.format = new_format
        self.config['format'] = new_format

    def update_hash(self):
        self.sha_digest = dirhash(self.data_path, 'sha256')
        self.config['sha_256'] = self.sha_digest

    def list_files(self, output_path):
        """
        Introduces the possibility to list all files and their hashes from a certain dataset. Might be useful when
        we'll be facing duplicities when dumping more datasets (with possibly overlapping files).
        :param output_path:
        :return:
        """
        dct = {'records': []}
        for sample in self.samples:
            relative_path = os.path.relpath(sample, self.data_path)
            dct['records'].append({relative_path: helpers.hash_file(sample)})
        with open(output_path, 'w') as handle:
            yaml.dump(dct, handle)
