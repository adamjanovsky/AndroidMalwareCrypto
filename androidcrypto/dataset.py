import androidcrypto.constants as constants
from androguard.core.analysis import auto
from checksumdir import dirhash
from hashlib import sha256
import lzma
import pickle
import os
import logging
import yaml
from os.path import *
import lzma
from androguard.core.analysis import analysis
from androguard import misc

from multiprocessing.dummy import Pool as ThreadPool

class DatasetException(Exception):
    pass


class Dataset:
    """
    A dataset is expected to be stored in a folder with the following structure:
    root
    |- meta.yml (metafile containing information about the dataset
    |- Data (arbitrary subfolders with apks)
    ---| - apk1
    ---| - Subfolders
    If no sha-digest is set in meta.yml, new is computed from the Data folder.
    It is however assumed that once hash is computed, it is not being changed - we should rethink this
    TODO: Rethink the hash paradigm above
    """
    def __init__(self, root_path):
        self.root_path = root_path
        self.meta_path = os.path.join(self.root_path, constants.META_FILENAME)
        self.data_path = os.path.join(root_path, constants.DATA_DIRNAME)

        self.sha_digest = None
        self.n_samples = 0
        self.name = None
        self.description = None
        self.format = None

        self.samples = []
        self.parse_meta()

    def generator(self):
        """
        Use this to iterate over dataset. Returns the Analysis object if successful, None otherwise.
        """
        for sample_path in self.samples:
            if self.format == 'apk':
                yield self.load_apk(sample_path)
            elif self.format == 'lzma':
                yield self.load_lzma(sample_path)

    @staticmethod
    def load_apk(path):
        """
        Decompile path to the apk file into Analysis object.
        """
        try:
            a, d, dx = misc.AnalyzeAPK(path)
            return dx
        except Exception:
            return None

    @staticmethod
    def load_lzma(path):
        """
        Decompress, deserialize the path to the Analysis object.
        """
        try:
            with lzma.open(path, 'rb') as handle:
                return pickle.load(handle)
        except Exception as e:
            return None

    def parse_meta(self):
        """
        Parses the meta.yml dataset file and constructs a Dataset object out of it.
        :return:
        """
        if not os.path.isfile(self.meta_path):
            raise DatasetException('Dataset meta file not found on the path')

        with open(self.meta_path, 'r') as meta_file:
            config = yaml.load(meta_file, Loader=yaml.FullLoader)
            self.name = config['dataset_name']
            self.description = config['description']
            self.format = config['format']

            self.sha_digest = None if config['sha_256'] == 'None' else config['sha_256']
            if self.sha_digest is None:
                self.sha_digest = dirhash(self.data_path, 'sha256')
                config['sha_256'] = self.sha_digest

            self.n_samples = config['n_samples']

            to_update = False
            if self.n_samples == 0:
                to_update = True
            for root, dir, files in os.walk(self.data_path):
                dir_apks = [os.path.join(root, f) for f in files if f.endswith(self.format)]
                if to_update:
                    self.n_samples += len(dir_apks)
                self.samples.extend(dir_apks)
            config['n_samples'] = self.n_samples

        with open(self.meta_path, 'w') as meta_file:
            yaml.dump(config, meta_file)

    def update_meta_format(self, new_format):
        """
        Updates a record of format key in meta.yml file
        :param new_format: new format to set.
        :return:
        """
        with open(self.meta_path, 'r') as meta_file:
            config = yaml.load(meta_file, Loader=yaml.FullLoader)
            config['format'] = new_format
            config['sha_256'] = dirhash(self.data_path, 'sha256')

        with open(self.meta_path, 'w') as meta_file:
            yaml.dump(config, meta_file)

    def update_hash(self):
        """
        Update hash of the dataset
        """
        self.sha_digest = dirhash(self.data_path, 'sha256')


class ParallelDecompiler:
    """
    Parallel decompiler. Stars n_threads threads and decompiles the files into lzma compressed analysis objects
    """
    def __init__(self, samples, base_input_path, output_path, n_threads):
        self.samples = samples
        self.input_path = base_input_path
        self.output_path = output_path
        self.n_threads = n_threads

    def worker(self, path_to_apk):
        a, d, analysis_object = misc.AnalyzeAPK(path_to_apk)

        relative_dir = dirname(relpath(path_to_apk, self.input_path))
        new_dirname = join(self.output_path, relative_dir)
        new_basename = str(basename(path_to_apk).split('.apk')[0]) + '.lzma'
        new_path = join(new_dirname, new_basename)

        if not os.path.exists(new_dirname):
            os.makedirs(new_dirname)

        if len(analysis_object.classes) > 0:
            print(analysis_object)
            with lzma.open(new_path, 'wb') as handle:
                pickle.dump(analysis_object, handle)

    def process_dataset(self):
        pool = ThreadPool(8)
        pool.map(self.worker, self.samples)
